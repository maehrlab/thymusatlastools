
---
title: "Cleaning the DGE Data"
author: "Eric Kernfeld"
date: "September 7, 2016"
output: html_document
---

###Setup 

This chunk sets up the file system and some functions to retrieve reference tables. It also loads in some useful packages and sets up a few hand-picked color schemes. 

```{r}

PATH_TO_DATA = file.path( "Data" )
PATH_TO_METADATA = file.path( PATH_TO_DATA, "metadata.csv")
PATH_TO_TABLES = file.path( "tables" ) 

library(enrichR)
library(grid)
library(gridExtra)
library(colorspace)
library(Seurat)
library(ggplot2)
library(reshape2)
library(dplyr)
library(magrittr)
library(reshape)
library(monocle)
library(hexbin)
library(cluster)
library(mgcv)
library(destiny)
assertthat::assert_that( packageVersion("destiny") >= "2")
if( packageVersion("ggplot2") < "2.2.1.9000" ) { 
  warning("You may need the development version of ggplot2 if you want the 
          `overplot_adjust=T` option in `custom_feature_plot` to work.")
}


get_rene_markers = function(){
  markers_df = read.csv( file.path( PATH_TO_TABLES, "cell_type_markers.csv" ), stringsAsFactors = T )
  if( !all( markers_df$marker == Capitalize( markers_df$marker ) ) ){
    warning( paste0( "Not all genes in ", 
                     file.path( PATH_TO_TABLES, "cell_type_markers.csv" ), 
                     " are Capital Case." ) )
  }
  return( markers_df )
}

#' Return a table of genes generated by head-to-head comparison of cTECs and mTECs on the Atlas data.
#'
get_cTEC_mTEC_genes = function(){
  active_genes = read.table( file.path( PATH_TO_TABLES, "cTEC_mTEC_data_driven.txt" ),
                             sep = "\t", stringsAsFactors = F )
  return( active_genes )
}

get_ramilowski = function(){
  ramilowski = read.table( file.path( PATH_TO_TABLES, "LigandReceptor_Ramilowski2015_mouse.txt" ), 
                         header = T, sep="\t", stringsAsFactors = F )
  return( ramilowski )
}

get_macosko_cc_genes = function( case = "Capital" ){
  cc_genes = read.csv( file.path( PATH_TO_TABLES, "Macosko_cell_cycle_genes.txt" ), 
                       sep = "", header = T, stringsAsFactors=FALSE )
  colnames_temp = colnames( cc_genes )
  if( case == "Capital" ){
    cc_genes = apply( cc_genes, MARGIN = 2, FUN = Capitalize )
  } else if ( case == "UPPER" ) {
    cc_genes = apply( cc_genes, MARGIN = 2, FUN = toupper )
  }
  colnames( cc_genes ) = colnames_temp
  return ( cc_genes )
}

get_mouse_tfs = function( capitalization = "all" ){
  mouse_tfs = read.table( file.path( PATH_TO_TABLES, "mouse_tfs.txt" ), header = T )$symbol %>% as.character
  if( capitalization == "all" ){
    mouse_tfs = c( mouse_tfs, Capitalize( mouse_tfs ), toupper( mouse_tfs ) )
  } else if( capitalization == "UPPER" ){
    mouse_tfs =  toupper( mouse_tfs )
  } else if( capitalization == "Capitalized" ){
    mouse_tfs = Capitalize( mouse_tfs )
  } else if( capitalization == "unchanged" ){
    # don't change it!
  }
  return( mouse_tfs )
}

get_poised_genes = function(){
  x = read.table( file.path( PATH_TO_TABLES, "Lesch2016_NatGen_PoisedGenes.txt" ), 
                  header = T, stringsAsFactors = F )
  return( x[["mouse_gene_symbol"]] )
}

get_ortholog_table = function(){
  ortholog_table_path = file.path( PATH_TO_TABLES, "mousegene_matchedHumanOrth.txt" )
  if( !file.exists( ortholog_table_path ) ){
    stop( "Please put a table of human-mouse orthologs at PATH_TO_TABLES/mousegene_matchedHumanOrth.csv. It should be a CSV file with columns named `mousesym` and `humansym` containing mouse and human gene symbols. ")
  }
  return( read.csv( ortholog_table_path, header = T, stringsAsFactors = F ) )
}

get_ATAC_seq = function(){
  the_right_file = "mm9.2.e13.5Embryo_ATAC_summits_350bp_allChroms_regPotential.txt"
  atac = read.table( file.path( PATH_TO_ATAC, the_right_file ), header = T, stringsAsFactors = F )
  # colnames( atac ) = lapply( colnames( atac ), FUN = substring, first = 3)
  ensembl_ids = read.table( file.path( PATH_TO_TABLES, "mouse_genes_ensembl.txt" ),
                            header = T, stringsAsFactors = F, sep = "\t" )
  ensembl_ids = ensembl_ids[!duplicated( ensembl_ids$Gene.ID ), ]
  
  # # Get rid of genes present in only one table or the other
  missing_from_biomart_table = setdiff( rownames( atac ), unique( ensembl_ids$Gene.ID ))
  print( paste0("Discarding ", length( missing_from_biomart_table ),
               " genes that are in Michael's data but not available through BioMart." ) )
  genes_use_ensembl = intersect( rownames( atac ), unique( ensembl_ids$Gene.ID ) )
  atac_convertible = atac[ genes_use_ensembl, ]
  ensembl_ids = subset( ensembl_ids, Gene.ID %in% genes_use_ensembl )

  atat( !any( is.na( atac_convertible ) ) )
  
  my_map = setNames( ensembl_ids$Associated.Gene.Name, nm = ensembl_ids$Gene.ID )
  preimage_info = get_preimage( my_map, detailed_output = T )
  atac_converted_unique = atac_convertible[names( preimage_info$output_occurs_once ), ]
  rownames( atac_converted_unique ) = preimage_info$output_occurs_once 
  atat( !any( is.na( atac_converted_unique ) ) )
  atac_converted_dupes = matrix(NA, 
                                ncol = ncol( atac_converted_unique ), 
                                nrow = length( unique( preimage_info$output_occurs_multiple ) ) )
  colnames( atac_converted_dupes ) = colnames( atac_converted_unique )
  dupe_symbols = unique( preimage_info$output_occurs_multiple ) 
  rownames( atac_converted_dupes ) = dupe_symbols
  
  print( paste0( "Converting ", 
                 length( my_map ),
                 " ensembl ids to only ",
                 length( preimage_info$preimage ),
                 " gene symbols. Colliding rows will be averaged. " ) )
  for( gene_symbol in dupe_symbols ){
    idx = preimage_info$preimage[[ gene_symbol ]]
    atac_converted_dupes[gene_symbol, ] = colMeans( atac_convertible[idx, ])
  }

  atac_converted = rbind( atac_converted_unique, atac_converted_dupes )
  atat( !any( is.na( atac_converted ) ) )

  return( atac_converted )
}

showcol = function(col) { pie(rep(1, length(col)), col = col) }
CC_PHASES = c("IG1.S", "S", "G2.M", "M", "M.G1")
black_white = c( colorspace::sequential_hcl( 30, h = 0,   c. = c(0, 0),     l = c( 0, 100  ) ) )
blue_gray_red  = colorspace::diverge_hcl   ( 30,          c = 180,          l = c( 40, 80  ) )
blue_purple_red = colorRampPalette(c("blue", "red"))(30) 
red_yellow  = c( colorspace::sequential_hcl( 15, h = 0,   c. = c(150, 0),   l = c( 40, 30  ) ),
                 colorspace::sequential_hcl( 15, h = 40,  c. = c(0, 80),    l = c( 30, 80  ) ) )
yellow_red  = c( colorspace::sequential_hcl( 18, h = 60,  c. = c(100, 100), l = c( 100, 80 ) ),
                 colorspace::sequential_hcl( 12, h = 0,   c. = c(100, 160), l = c( 80, 40  ) ) )
blue_yellow = c( colorspace::sequential_hcl( 15, h = 260, c. = c(50, 0),    l = c( 13, 65 ) ), 
                 colorspace::sequential_hcl( 15, h = 40,  c. = c(0, 80),    l = c( 65, 95 ) ) )
human_mouse_vvv_colors = c( "goldenrod4", "lightgoldenrod2", "goldenrod3", "goldenrod1", 
                            "cadetblue4", "blue4", "cadetblue3", "blue1", "cadetblue1", 
                            "firebrick4","deeppink3", "firebrick3","palevioletred1", "firebrick1")
Thanksgiving_colors = c("yellow", "orange", "red", "brown")
imitate_cubehelix = c("#fff6e3",
                      "#ddcac4",
                      "#bca0a6",
                      "#9b7789",
                      "#7b506d",
                      "#5b2a52",
                      "#3c0038")
```

###Utilities

```{r}
  
atat = function(my_ass) (assertthat::assert_that(my_ass))
atae = function(x, y, ...) (assertthat::are_equal(x, y, ...))

# This function plots two variables, labeling the ones that are far from their neighbors.
# `data` should be a dataframe with first column to go on the horizontal axis,
#    second on the vertical. The third column is treated as the labels and the
#    fourth (optional) is treated as the color.
# `main` shows up as the figure title.
# The proportion of points labeled is `prop_label`.
# Prints and returns a ggplot object.
outlier_labeled_scatterplot = function( data, main = "", prop_label = 0.02 ){
  
  # sanitize input
  if(any(is.na(data))){
    cc = complete.cases( data )
    warning(c("Removing ", sum(1-cc), " rows with missing data."))
    data = data[cc, ]
  }
  
  cc = is.finite(data[, 1]) & is.finite(data[, 1]) 
  if(any(!cc)){
    warning(c("Removing ", sum(1-cc), " rows with NaN, Inf, or -Inf."))
    data = data[cc, ]
  }

  
  x = names(data)[[1]]
  y = names(data)[[2]]
  label = names(data)[[3]]
  colour = NULL
  if( length( data ) > 3 ){ colour = names(data)[[4]] }
  data$dist_to_nn = rowSums( FNN::get.knn( data = apply(X = data[ , 1:2 ], MARGIN = 2, FUN = div_by_max ),
                                           k=3, algorithm=c( "cover_tree" ) )$nn.dist )
  data$should_label = data$dist_to_nn > quantile( data$dist_to_nn, probs = (1 - prop_label))
  
  p = ggplot() + ggtitle( main ) +
    geom_text(  data = subset( data,  should_label ),
                aes_string( x = x, y = y, colour = colour, label = label ) ) +
    geom_point( data = subset( data, !should_label ),
                aes_string( x = x, y = y, colour = colour ) )
  print( p )
  return( p )
}

distance_sq = function( x, y ) { sum( ( x - y )^2 ) }
nnz = function(x)(sum(x>0))
prop_nz = function(x)( nnz(x) / length(x))

#' Quickly define an aggregator function that works only on positive (or nonzero) elements.
#' 
#' @param FUN function that accepts a numeric vector.
#' @param default If nonzerify(FUN, default) is applied to a list of ineligible elements, it returns this.
#' @param allow_negative If F (default), returned function only acts on positive numbers. Otherwise, all nonzeroes are allowed in.
#' @value A function that returns the result of applying FUN to only the eligible values.
nonzerify = function( FUN, default = 0, allow_negative = F ){
  if( allow_negative ) {
    eligibles = function( x ) (x != 0)
  } else {
    eligibles = function( x ) (x >= 0)
  }
  to_return = function( x ){
    if( all( x==0 ) ){return(0)}
    return( FUN( x[x>0] ) )
  }
}
atae( nonzerify(median)(-1:5), 3)
atae( nonzerify(mean)(-1:5), 3)
atae( nonzerify(prop_nz)(c(0,0,0,4)), 1)

div_by_max = function( x ){ return( x / max( x ) ) }
div_by_sum = function( x ){ return( x / sum( x ) ) }
percentify = function( x ){ return( 100*round( div_by_sum( x ), 3 ) ) }
standardize = function( x, nonpar = F ){
  if(nonpar){
    y = x - median(x)
    fake_sd = IQR( y ) / 1.37
    z = y / fake_sd
  } else {
    y = x - mean(x)
    z = y / sd( y )
  }
  return( z )
}

# # Given a matrix, scatterplots all pairs of columns.
plot_pairs = function( X, main = "" ){
  D = ncol( X )
  for( ii in 1:D ){
    for( jj in 1:ii ){
      # linear_index = (ii - 1)*D + jj
      if( ii == jj ){
        # could do histogram here, but I don't want to
      } else {
        Y = data.frame( X[[ii]], X[[jj]], rownames( X ) )
        colnames( Y ) = c( colnames( X )[ c(ii, jj) ], "gene" )
        r = cor(Y[, 1], Y[, 2])
        p =  ggplot( Y ) + ggtitle( paste0( main, " (r = ", round(r, 2), ")" ) ) + 
          geom_point( aes_string( x = colnames(Y)[[1]], y = colnames(Y)[[2]] )) 
        print( p ) 
      }
    }
  }
}


matrixify_preserving_rownames = function(x) matrix( x,  ncol = 1, dimnames = list( names( x  ), "") )
vectorize_preserving_rownames = function(x, i = 1) { v = x[, i]; names(v) = rownames(x); return(v) }
down_idx = function(x){ x[[1]] }
factor_numeric = function(x) {
  atat(is.numeric(x))
  x = factor(x, levels = sort(unique(x)), ordered = T)
}

# Like dplyr::top_n but it preserves the rownames.
top_n_preserve_rownames = function( x, ...){
  if(is.null(rownames(x))){return(top_n(x, ...))}
  rownames_tempcol = make.unique( c( colnames( x ), "rownames_tempcol" ) ) %>% rev %>% down_idx
  x[[rownames_tempcol]] = attr(x, "row.names")
  y = top_n(x, ...)
  attr(y, "row.names") = y[[rownames_tempcol]]
  y[[rownames_tempcol]] = NULL
  return(y)
}
# Make sure an adversarial case -- temp column name already taken -- works out ok
atat( all.equal(  top_n_preserve_rownames(x = data.frame(rownames_tempcol = 10:6), 3, rownames_tempcol), 
                  data.frame(rownames_tempcol = 10:8) ) )

# # Friendlier version of aggregate(). Accepts atomic "by" argument.
# # Guaranteed to return a dense dataframe. Also returns the aggregation levels in the rownames 
# # instead of adding a fucking column for them.
aggregate.nice = function(x, by, FUN, ... ) {
  if( typeof( by ) != "list" ){
    by = list ( by )
  } 
  right_type = ( is.atomic(x) | typeof( x ) %in% c( "matrix", "dataframe" ) )
  if( !right_type ){
    x = as.matrix( x )
  }
  result = aggregate( x, by, FUN = function(x) FUN(x, ...) )
  rownames( result ) = as.character( result[, 1] )
  result = result[ , -1, drop = F]
  return( as.matrix( result ) )
}
test1 = data.frame(1:6, 2, 3)
atae( aggregate.nice( test1, letters[1:6], mean ), test1 )
atat( all( aggregate.nice( test1, rep("A", 6), sum ) == matrix( c(21, 12, 18), nrow = 1) ) )

replace_with_int_rank = function(x) as.numeric( as.factor( x ) )
atat(all(replace_with_int_rank(1:5) == 1:5))
atat(all(replace_with_int_rank(c("a", "b", "a", "c", "c")) == c(1, 2, 1, 3, 3)))
atat(all(replace_with_int_rank(c("0", "2", "0", "4", "4")) == c(1, 2, 1, 3, 3)))
  
na2zero = function(df){
    df[is.na(df)] <- 0
    return(df)
}

Capitalize = function(s) {paste0(toupper( substring( s, 1, 1 ) ), tolower( substring( s, 2 ) ) )}
assertthat::are_equal(Capitalize(c("FOXN1", "PSMB11")), c("Foxn1", "Psmb11") )

# Turn a named list or vector of strings into a 
# pipe-separated key-value format: "<name1>=<value1>|<name2>=<value2>|..."
collapse_by_name = function(named_list){
  name_eq_element = named_list # preallocate
  for(field in names( named_list ) ){
    name_eq_element[[field]] = paste0(field, "=", named_list[[field]])
  }
  return(paste(name_eq_element, collapse = "|"))
}

# Removes a suffix if it is present without damaging the string when it's not present.
# Not vectorized!
strip_suffix = function(s, suffix){
  if( !is.atomic( s ) ) { stop( "This function isn't vectorized. Sorry!" ) }
  if( length( s ) != 1 ){ stop( "This function isn't vectorized. Sorry!" ) }
  nc = nchar(s)
  ncs = nchar(suffix)
  if( substring(s, nc + 1 - ncs, nc ) == suffix ){ s = substring(s, 1, nc - ncs ) }
  return(s)
}
atae( strip_suffix("blah.pdf", ".pdf"), "blah")
atae( strip_suffix("blah",     ".pdf"), "blah")

text2file = function(filename, vector_of_lines){
  vector_of_lines = as.character(vector_of_lines)
  fileConn<-file(filename)
  writeLines(vector_of_lines, fileConn)
  close(fileConn)
}

dir.create.nice = function(my_dir){
  if( !dir.exists( my_dir ) ){ dir.create( my_dir, recursive = T) }
}

# Splits out the folder names and returns them in reverse order with "" at the end.
# split_path("/Users/erickernfeld/Dropbox/2016JULY07scRNAseq/") yields:
# "2016JULY07scRNAseq"      "Dropbox"            "erickernfeld"       "Users"              ""  
split_path = function(path) {
  if (dirname(path) %in% c(".", path)) return(basename(path))
  return(c(basename(path), split_path(dirname(path))))
}

# # Takes in a function from one finite set of strings to another
# # in the form of a named vector `map`. (Inputs are names, outputs are values.)
# # 
# # If `detailed_output`, returns a list with named elements:
# #   - `preimage`: a list where each name is an output (a value of `map`)
# #       and each element is a vector of all inputs leading to that output.
# #   - `output_occurs_multiple`: a named list or vector subsetted from `map` 
# #       where each element occurs more than once.
# #   - `output_occurs_once`: a named list or vector subsetted from `map` where 
# #       each element occurs once.
# # Otherwise, it returns just the preimage.
get_preimage = function( map, detailed_output = F ){
  if( any( is.na( map ) ) ) { warning( "I haven't tested this on missing values." ) }
  # # Preallocate. Empty lists are treacherous bastards, so fill in NA's at first.
  range_of_map = unique( map )
  preimage = setNames( as.list( rep(NA, length( range_of_map ) ) ), nm = range_of_map )
  
  # # Save CPU time by setting the easy ones wholesale
  output_occurs_multiple = map[ map %in% map[ duplicated( map ) ] ]
  output_occurs_once = map[ !( map %in% output_occurs_multiple ) ]
  preimage[ output_occurs_once ] = names( output_occurs_once )
  
  # # Fill in fibers
  for( input in names( output_occurs_multiple ) ){
    preimage[[ map[[ input ]]  ]] = c( preimage[[ map[[ input ]]  ]], input )
  }
  # # Clean up NA's one by one
  remove_NA = function( x ){ x[!is.na(x)] }
  preimage = lapply( preimage, FUN = remove_NA )
  atae( length( output_occurs_once ), 
        length( unique( output_occurs_once ) ) )
  if(!detailed_output){ return( preimage ) }
  return( list( preimage = preimage, 
                output_occurs_multiple = output_occurs_multiple, 
                output_occurs_once = output_occurs_once ) )
}
atae( get_preimage( map = setNames( LETTERS, letters) ), as.list( setNames( letters, LETTERS) )  )
atae( get_preimage( map = setNames(      c("A", "B", "DUPE", "DUPE"), 
                                    nm = c("a", "b", "c",    "d") ) ), 
      list(A = "a", 
           B = "b",
           DUPE = c( "c", "d" ) ) ) 


#' Get available variable names (genes, identity classes, PCA embeddings, etc)
#'
#' @param object Seurat object
#' @return Returns a character vector of all eligible inputs to the `vars.all` argument of `FetchData`.
#' @export
AvailableData = function( object ){
  available_categorized = list( metadata = names( object@data.info ),
                                PCs = names(object@pca.x),
                                tsne = names(object@tsne.rot),
                                ICs = names(object@ica.rot),
                                genes = rownames( object@data ),
                                ident = "ident" )
  return( Reduce( f = union, x = available_categorized ) )
}


#' FetchData but with zeroes for unavailable genes
#'
#' @export
#' @param dge Seurat object
#' @param vars.all List of all variables to fetch. Missing entries are ignored.
#' @param ... Other arguments to pass to FetchData
#'
#' @details This function is stupid: if you ask for "PC1" and it's not available,
#' it will think you're asking for a non-expressed gene, so it will return zeroes.
FetchDataZeroPad = function( dge, vars.all, ... ){
  vars.all = vars.all[complete.cases(vars.all)]
  avail = intersect( vars.all, AvailableData( dge ) )
  unavail = setdiff( vars.all, AvailableData( dge ) )
  to_return  = FetchData( dge,  avail, ... ) 
  pad = as.data.frame( matrix(0,           
                              nrow = nrow( to_return ), 
                              ncol = length( unavail ),
                              dimnames = list( rownames( to_return ),               
                                               unavail) ) )
  to_return = cbind( to_return, pad )
  assertthat::are_equal( sort( vars.all ),   sort( colnames( to_return ) ) )   
  to_return = to_return[, vars.all, drop = F]
  assertthat::assert_that( is.data.frame( to_return ) )
  return( to_return )
}

```

###Human-mouse conversion

```{r}
# # Set up data on human-mouse orthologs
ortholog_table = get_ortholog_table()
# # Hash tables for fast access 
human_dupes = duplicated( ortholog_table$humansym )
mouse_dupes = duplicated( ortholog_table$mousesym )
h2m = setNames( ortholog_table$mousesym, nm = ortholog_table$humansym )[!human_dupes]
m2h = setNames( ortholog_table$humansym, nm = ortholog_table$mousesym )[!mouse_dupes]

# # This function returns the ortholog of a given gene.
# # If it finds no match, it returns NA.
get_ortholog = function( gene, from = "human", to = "mouse" ){
  if       ( from == "human" && to == "mouse"){
    return( h2m[ gene ] )
  } else if( from == "mouse" && to == "human"){
    return( m2h[ gene ] )
  } else {
    warning(' The only working options are from = "human", to = "mouse" and from = "mouse", to = "human". Returning your gene unaltered. ')
    return( gene )
  }
}
# # Same as get_ortholog but returns just T or F.
has_ortholog = function( ... ){ !is.na( get_ortholog( ... ) ) }


# # This function converts a raw digital gene expression matrix from one species to another.
# # If two genes have the same ortholog, the molecule counts get merged.
# # If a gene has no ortholog, it is omitted.
# # The input must be a matrix with genes stored in rownames( raw_dge ).
convert_species_dge = function( raw_dge, from = "human", to = "mouse"){
  cat( paste( "Converting to", to, "...\n" ) )
  genes = rownames(raw_dge) 
  eligible_genes = genes[ has_ortholog( genes, from, to ) ]
  genes_by_ortholog = eligible_genes %>% get_ortholog( ., from, to ) %>% get_preimage
  raw_dge_converted = matrix( 0, nrow = length( genes_by_ortholog ), ncol = ncol( raw_dge ) )
  rownames( raw_dge_converted ) = names( genes_by_ortholog )
  colnames( raw_dge_converted ) = colnames( raw_dge )
  for( ortholog in names( genes_by_ortholog ) ){
    raw_dge_converted[ ortholog, ] = raw_dge[ genes_by_ortholog[[ortholog]], , drop = F ] %>% colSums
  }
  return( raw_dge_converted )
}

# # The input is a Seurat object with a metadata field "species".
# # Removes genes that appear in less than 3 out of 1000 cells of any species
# # Also removes genes where the proportion in one species is higher by 0.5 than the other.
remove_species_specific_genes = function( dge, results_path, threshold = 0.003, diff_thresh = 0.5 ){
  
  proportions_by_species = aggregate.nice( x   = as.matrix( t( dge@data > 0 ) ), 
                                           by  = dge@data.info$species , 
                                           FUN = mean )
  min_proportions_by_species = sapply( X = proportions_by_species, FUN = min)
  max_proportions_by_species = sapply( X = proportions_by_species, FUN = max)
  
  plot_df = as.data.frame( t( proportions_by_species ) ); names ( plot_df ) = rownames( proportions_by_species )
  plot_df$gene = rownames( plot_df )
  plot_df$diff_big = ( max_proportions_by_species - min_proportions_by_species > diff_thresh )
  plot_df$absent_in_one =  ( min_proportions_by_species < threshold )
  plot_df$excluded = plot_df$absent_in_one | plot_df$diff_big

  pdf( file.path( results_path, "min_proportions_by_species.pdf" ) )
  {
    hist( unlist( min_proportions_by_species[min_proportions_by_species < 10*threshold ] ), 
          main = "Min proportions truncated at 10*threshold",
          xlab = "Proportion in mouse or human (whichever is lower)")
    p = ggplot() + ggtitle("Proportion of cells expressing each gene") +
      geom_point( data = subset( plot_df, !diff_big | absent_in_one ),
                  aes_string( x = names ( plot_df )[1],
                              y = names ( plot_df )[2],
                              colour = "excluded" ) ) +
      geom_text( data = subset( plot_df, diff_big & !absent_in_one ), 
                 aes_string( x = names ( plot_df )[1],
                             y = names ( plot_df )[2],
                             label = "gene", 
                             colour = "excluded" ) )
    print( p )
  } 
  dev.off()

  passing_genes = plot_df$gene[ !plot_df$excluded ]
  print( paste0( "Removing ", 100*round( mean( plot_df$excluded ), 2), "% of the genes." ) )
  if( !is.null( dge@data       ) ) { dge@data       = dge@data      [passing_genes, ]}
  if( !is.null( dge@raw.data   ) ) { dge@raw.data   = dge@raw.data  [passing_genes, ]}
  if( !is.null( dge@scale.data ) ) { dge@scale.data = dge@scale.data[passing_genes, ]}
  if( !is.null( dge@mean.var   ) ) { dge@mean.var   = dge@mean.var  [passing_genes, ]}
  if( !is.null( dge@var.genes   ) ) { dge@var.genes %<>% intersect(  passing_genes )}
  return( dge )
}
```

###Useful sample groupings

#####Whole samples (`whole_e*`)

The goal here is to find out what cell subpopulations are present at the earliest stage of thymus formation. We expect to see thymus and parathyroid tissue, and we expect to cell epithelial cells, mesenchymal cells, and blood. This dataset contains three biological replicates from Venus mice dissected at embryonic day 12.5 and likewise for wildtype mice at day 13.5. UPDATE: added the day 14.5 data to the `whole_organ` samples on 2016 Nov 28.

These analyses are for a first pass. Once we identify some clusters of interest, we can load the data back in, subset them, and look for interesting subpopulations.

#####Time-series of sorted samples (`time_sorted_*`)

The goal here is to take a look at the days leading up to thymus specification. We want to see when various subpopulations start to become well-defined and what genes seem to be implicated. There are strong temporal effects, so in addition to looking at days 10.5-12.5 together, we look at them day by day. One analysis also adds in day 13.5.

#####In-vitro cell lines

These cell lines are from the lab's attempts at human and mouse thymus production in a dish. We want to compare them to the timeseries data and see where they fit in (and what to do next!).

```{r}
# Returns a list of character vectors named by sample type with entries containing 
# sample IDs of all replicates of that sample type. Omits in vitro data.
get_data_by_replicates = function(){
  replicates = get_sample_groups( c( "whole_e12_5",
                                     "whole_e13_5",
                                     "whole_e14_5", 
                                     "whole_e15_5", 
                                     "whole_e16_5",
                                     "time_sorted_10",    
                                     "time_sorted_11", 
                                     "time_sorted_12",   
                                     "time_sorted_13",   
                                     "time_sorted_16"), combine = F )
  return( replicates )
}

# If `combine==T`, then this function returns a single vector with all unique sample names.
# Otherwise (default), the return value is a list of vectors of sampleids that go together 
# for various analytic themes. These themes are reflected in the list's names.
# If you put in "whole_organ" or "sorted_timeseries" or "in_vitro", you get one of the datasets above.
# If you put in "all", you get everything from the embryonic mouse thymus.
# If you put in anything else, it gets used as a list index, and you'll have to RTFS (sorry!) 
# to see the options available. For example, if you just want the day 12.5 data from the whole 
# organ, put "whole_e12_5"; if you just want the human in-vitro cells, put "vitro_human".

get_sample_groups = function(analyses = "whole_organ", combine = F){
  
  whole_organ = list(whole_e12_5 = c( "e12_5wholeThy_venus",
                                      "e12_5wholeThy_venus_rep2",
                                      "e12_5wholeThy_venus_rep3" ),
                     whole_e13_5 = c( "e13_5wholeThy_wt_rep3",
                                      "e13_5_wholeThymus_wt_rep4",
                                      "e13_5_wholeThymus_wt_rep5" ),
                     whole_e13_5_pth = c( "e13_5wholeThy_wt_rep2",
                                          "e13_5wholeThy_wt" ),
                     whole_e14_5 = c( "E14_5_WholeThy_rep2",
                                      "E14_5_WholeThy_rep4" ),
                     whole_e15_5 = c( "E15_5_WholeThy_Rep1",
                                      "E15_5_WholeThy_Rep2" ),
                     whole_e16_5 = c( "e16_5_wholeThymus_wt_rep1",
                                      "E16_5_WholeThy_Rep2_combine",
                                      "E16_5_WholeThy_Rep3_combine" ),
                     whole_adult = c( "adult_thy_rep1",
                                      "adult_thy_rep2" ),
                     human_patient = c( "hThymus_C_Epcam",
                                        "hThymus_D_Epcam" ) ) 
  sorted_timeseries = list(time_sorted_9     = c( "e9_5",
                                                  "e9_5_rep2"),
                           time_sorted_10    = c( "e10_5",
                                                  "e10_5rep3" ),
                           time_sorted_11    = c( "e11_5rep3",
                                                  "e11_5rep4" ),
                           time_sorted_12    = c( "e12_5rep4",
                                                  "e12_5rep3" ),
                           time_sorted_13    = c( "e13_5rep2",
                                                  "e13_5rep3" ),
                           time_sorted_16 = c("e16_5EpCAM_rep1",
                                              "e16_5EpCAM_rep2") )
  in_vitro = list( optimization = c( "NoRNaseInh",
                                     "NoRNaseInh_1hour",
                                     "RNaseInh",
                                     "RNaseInh_1hour" ), 
                   vitro_mouse = c( "P2lox40_ESC_Live_1",
                                    "P2lox40_EPI_Live_1",
                                    "P2lox40_DE_Live_1",
                                    "P2lox40_6DPFE_Epcam",
                                    "P2lox40_14DPFE_Epcam"),
                   vitro_human = c( "hESC_rep2_seq2",
                                    "hESC_rep3_seq2",
                                    "hDE_H1_R",
                                    "hDE_rep3_seq2",
                                    "hAFE_rep1",
                                    "hAFE_rep2_seq2",
                                    "hPFE_rep2",
                                    "hPFE_H1",
                                    "hTEP_rep2",
                                    "hTEP_H1",
                                    "hTEPnew2_rep1",
                                    "hTEPnew_rep1",
                                    "hTEP_Control_H1LucGFP" ) )
  
  # Avoid eight different `else if` thingies 
  if( combine ){
    merge_list_or_not = function( x ) return( Reduce( f = union, x = x ) )
  } else {
    merge_list_or_not = function( x ) return( x )
  }
  
  # return the right thing
  if( identical( analyses, "whole_organ")) {
    return( merge_list_or_not( whole_organ ) )
  } else if( identical(analyses, "sorted_timeseries" )){
    return( merge_list_or_not( sorted_timeseries ) )
  } else if( identical( analyses, "in_vitro" ) ){
    return( merge_list_or_not( sorted_timeseries ) )
  } else if( identical( analyses, "all" ) ){
    return( merge_list_or_not( c(whole_organ, sorted_timeseries, in_vitro ) ) )
  } else {
    to_return = c( whole_organ, sorted_timeseries, in_vitro ) [analyses]
    if(length(to_return) == 0){ warning("No samples matched the analytic theme given.") }
    return( merge_list_or_not( to_return ) )
  }
}

# # This function returns a giant Seurat object containing the in-vivo versus in-vitro dataset.
# # It has a metadata field cell_type that gives either eday (if in vivo) or e.g. "PFE_6", "DE".
get_vivo_versus_vitro_data = function( results_path, test_mode = F, include_human = F ){
  if( include_human ){
    vivo_vs_vitro = get_sample_groups( c( "time_sorted_10_13", "vitro_mouse", "vitro_human" ), combine = T )
    levels_in_order = c( "mESC", "mEPI", "mDE", "mPFE_6", "mPFE_14", 
                         "10.5", "11.5", "12.5", "13.5", 
                         "hESC", "hDE", "hAFE",  "hPFE", "hTEP")
  } else {
    vivo_vs_vitro = get_sample_groups( c( "time_sorted_10_13", "vitro_mouse" ), combine = T )
    levels_in_order = c( "mESC", "mEPI", "mDE", "mPFE_6", "mPFE_14", 
                         "10.5", "11.5", "12.5", "13.5" )
  }
  raw_dge = dge_merge_list( load_thymus_profiling_data( sample_ids = vivo_vs_vitro, test_mode = test_mode ) )
  dge = seuratify_thy_data( raw_dge, results_path, test_mode = test_mode )
  dge = add_maehrlab_metadata( dge )
  dge = add_maehrlab_metadata( dge, variable_to_add = "source_tissue" )
  dge = add_maehrlab_metadata( dge, variable_to_add = "species" )
  
  # # The next few lines assume that the only in vivo data is the mouse embryo data.
  in_vivo_indicator = ( dge@data.info$source_tissue %in% c( "embryo" ) )
  cell_type                      = as.character( dge@data.info$source_tissue )
  cell_type[ in_vivo_indicator ] = as.character( dge@data.info$eday[ in_vivo_indicator ] )
  names( cell_type ) =   rownames( dge@data.info )
  dge = Seurat::AddMetaData( dge, metadata = cell_type, col.name = "cell_type")
  dge@data.info$cell_type = factor(dge@data.info$cell_type, 
                                   levels = levels_in_order,
                                   ordered = T) 
  if( include_human ){
    dge = remove_species_specific_genes( dge, results_path = file.path( results_path, "QC" ) )
  }
  return(dge)
}
```



###Loading and merging datasets

This chunk loads the data into a list of matrices. There is one array for every replicate. Each array has one column per gene and one row per cell. Strictly speaking, it's one row per droplet/barcode, and each droplet may have one cell, none, or two cells. These doublets occur about 5% of the time, given our setup. Test mode returns a teensy little dataset with 50 cells and 200 genes. 

For each id, it looks in the csv at `PATH_TO_METADATA` in the corresponding row and the column `dge_path`.

The column names are sample id's concatenated onto cell barcodes, separated by the infix `"|"`. That's ugly and big, but also unambiguous; it allows sample ID's to contain underscores without confusing Seurat when it goes to fetch the sample ID for `orig.ident`.

If `convert_all_to_mouse` (`TRUE` by default) then the human datasets are converted using `convert_species_dge`.
If `get_readcounts` (`FALSE` by default) then a readcount summary file is returned instead of a full DGE.

```{r}
load_thymus_profiling_data = function(sample_ids = "all", test_mode = F, convert_all_to_mouse = T ){
  
  # # Subset metadata down to just the desired samples
  metadata = read.csv( PATH_TO_METADATA, header = T, stringsAsFactors = F ) 
  if( length( sample_ids ) == 1 && sample_ids == "all" ){
    # Do nothing -- all metadata already present
  } else {
    metadata = subset( metadata, Sample_ID %in% sample_ids)
  }
  missing = setdiff( sample_ids, metadata$Sample_ID )
  if(length(missing) > 0){
      cat( "\n There are no records of these sample IDs: \n"); cat( missing ); cat("\n")
  }
  
  # # Check dge file availability 
  file_type_header = "dge_path"	
  available = file.exists( metadata[[ file_type_header ]] ) 
  sample_ids = as.character( metadata[["Sample_ID"]] )
  unavailable_samples_str = paste0( sample_ids[!available], collapse = "\n" )
  available_samples_str   = paste0( sample_ids[ available], collapse = "\n" )
  if(!all(available)){
    cat( "\n Files unavailable for these samples: \n"); cat( unavailable_samples_str ); cat("\n")
  }
  cat( "\n Proceeding with these samples: \n")      ; cat(   available_samples_str ); cat("\n")
  metadata = metadata[ available, ]
  runs_to_return = as.list( metadata[["Sample_ID"]] )
  names( runs_to_return ) = runs_to_return
  
  # # Fetch data
  for( i in seq_along( runs_to_return ) ){
    print( paste( "Loading dataset", metadata[i, "Sample_ID"] ) )
    readpath = metadata[i, file_type_header]
    dge = read.table( file = readpath, sep="\t", header=TRUE, row.names=1 )
    colnames( dge ) = paste( colnames( dge ), metadata[i, "Sample_ID"], sep = "|" )
    if( test_mode ){
      genes_included = c( rownames( dge )[1001:1200] )
      dge = dge[genes_included, 1:50]
    }
    # # Convert human data into mouse orthologs
    if( metadata[ i, "species" ] == "human" && convert_all_to_mouse ) {
      dge = convert_species_dge( dge ) 
    } 
    runs_to_return[[i]] = as.matrix( dge )
  }
  assertthat::are_equal( length( runs_to_return ), length( sample_ids) ) 
  return( runs_to_return )
}

load_bulk_data = function(){
  warning("Sorry, this function is still under development.")
  return()
  PATH_TO_BULK_DATA = file.path("~", "Dropbox", "shared_Bioinformatics", "bulkcell_RNAseq", 
                                "??????" )
  all_bulk_data = read.delim( PATH_TO_BULK_DATA, header = T, stringsAsFactors = F )
  
  # discard some undesirable data
  runs_to_keep = c("mAFE_smart", "mDE_smart", "mEPI_smart", "mESC_smart", 
                      "mAFE_smart_rep2", "mDE_smart_rep2", "mESC_smart_rep2", "mEpi_smart_rep2", 
                   "e9_5_rep1", "e9_5_rep2", "e10_5__new_rep3", "e10_5__new_rep4", 
                      "e11_5__new_rep3", "e11_5__new_rep4", "e12_5__new_rep3", "e12_5__new_rep4", 
                   "cTECrep1", "cTECrep2", "mTEChirep1", "mTEChirep2", "mTEClowrep1", "mTEClowrep2", 
                   "Brain", "Eye", "Heart", "Kidney", "Liver", "Lung",
                      "Salivary_Gland", "Smooth_Muscle", "Spinal_Cord", "Spleen")
  currently_useful = all_bulk_data[, c( "gene", runs_to_keep )]
  
  # Merge counts from different isoforms
  rownames( currently_useful ) = currently_useful$transcript
  duplicated_genes = unique( currently_useful$gene[duplicated(currently_useful$gene)] )
  expr_vals_by_gene = currently_useful[!duplicated(currently_useful$gene), ] 
  rownames( expr_vals_by_gene ) = make.unique( expr_vals_by_gene$gene )
  expr_vals_by_gene = expr_vals_by_gene[-1] %>% as.matrix
  for( gene in duplicated_genes ){
    indices = (currently_useful$gene == gene)
    expr_vals_by_gene[gene, ] = colSums( currently_useful[ indices, -1] )
  }
  
  # normalize to transcripts per 10,000
  total_desired = 10^4
  expr_vals_by_gene = apply( X = expr_vals_by_gene, MARGIN = 2, FUN = div_by_sum ) * total_desired

  return( expr_vals_by_gene )
}

```

`dge_merge_list` converts the digital gene expression matrices to dataframes with genes as columns, merges them, then converts the result back, all without disturbing the gene labels. Duplicate barcodes will cause errors, but the loading function earlier in the pipeline appends the sample ID to the barcodes, and that should make them unique. A gene will be included even if it only appears in one of the datasets. If a gene appears in one dataset but not another, zeroes will be filled in for missing expression levels.

```{r}

dge_merge_list = function(dge_list){
  # Allow duplicate cells but not duplicate genes
  all_genes = Reduce( f = union, x = lapply( dge_list, rownames ) )
  all_cells = Reduce( f = c,     x = lapply( dge_list, colnames ) )
  if(anyDuplicated(all_cells)){ warning( "Duplicate cell barcodes present." )}
  new_dge = as.data.frame( matrix( 0, nrow = length( all_genes ), 
                                      ncol = length( all_cells ) ) )
  rownames( new_dge ) = all_genes
  colnames( new_dge ) = all_cells
  for( ii in seq_along( dge_list ) ){
    dge = dge_list[[ii]]
    new_dge[rownames(dge), colnames(dge)] = dge
    print( paste( "Finished merging dge matrix", names(dge_list)[[ii]] ) )
  }
  #Results should have at least as many genes as the input with the most genes
  #the number of barcodes should equal the sum of the total barcodes.
  input_dimensions = Reduce(rbind, lapply(dge_list, dim))
  assertthat::assert_that(dim(new_dge)[1] >= max(input_dimensions[,1]))
  assertthat::assert_that(dim(new_dge)[2] == sum(input_dimensions[,2]))
  assertthat::assert_that(!is.null(colnames(new_dge)))
  assertthat::assert_that(!is.null(rownames(new_dge)))
  return( new_dge )
}

```

These functions extract information from the sample ID about embryonic day and replicate.

```{r}

# # This looks in PATH_TO_METADATA for extra information about our experiments.
# # It looks for a column named `variable_to_add` and adds that to @data.info with the 
# # name `new_name` (default is `variable_to_add`).
# # It relies on being able to match "Sample_ID" to "orig.ident".
add_maehrlab_metadata = function( dge, variable_to_add = "eday", new_name = NULL, NA_strings = c("NA", "") ){
  
  # # Get data from Maehrlab CSV file
  maehrlab_metadata = read.csv( PATH_TO_METADATA, header = T, stringsAsFactors = F ) 
  atat( variable_to_add %in% names( maehrlab_metadata ))
  data_by_sample = maehrlab_metadata[[variable_to_add]]
  data_by_sample[ data_by_sample %in% NA_strings ] = NA
  names( data_by_sample ) = maehrlab_metadata[["Sample_ID"]]
  
  # # Expand it to go cell by cell instead of sample by sample; add it to the Seurat object
  new_temp = data_by_sample[ as.character( dge@data.info$orig.ident ) ]
  names( new_temp ) = rownames( dge@data.info )
  if( is.null( new_name ) ){ new_name = variable_to_add }
  dge = Seurat::AddMetaData( dge, metadata = new_temp, col.name = new_name )
  return( dge )
}

# # We use the convention that the sample ID, which goes into the orig.ident variable,
# # looks like "e13_5_<irregular>".
# # This function extracts the embryonic day from the first three characters and 
# # forms labels with a consistent way of labeling replicates.
custom.make.unique = function(s, ...){
  label_dupes = function( s2, my_seq, seqgen = seq_along, sep = "_", do_singletons = F ) {
    if ( ( length(s2) == 1 ) && !do_singletons ) {
      s2
    }  else{
      paste(s2, seqgen(s2), sep = sep)
    }
  }
  f = function(x) label_dupes(x, ...)
  ave(as.character(s), s, FUN = f)
} 

```

###Basic QC

This function loads in all the raw data and checks for the number of UMIs, transcripts, and cells in each dataset. It is a screen for severe problems such as failed fluidics. It also looks at `Xist` and warns about potential female samples.

```{r}
check_all_scRNA = function( results_path ){
  metadata = read.csv( PATH_TO_METADATA, header = T, stringsAsFactors = F )
  
  # # Check to see the data is actually all there
  metadata$file_exists_auto = metadata$dge_path %>% file.exists 
  existence_status_correct = metadata$file_exists_auto == ( metadata$files_available == "yes" )
  if( !all( existence_status_correct ) ){
    print( "For these samples, the sheet says the files are available but they actually aren't, or vice versa." )
    print(" By row:" )
    cat( which( !existence_status_correct ) )
    print(" By Sample_ID:" )
    cat( metadata$Sample_ID[ !existence_status_correct ] )
  }
  
  # # Check for duplicate sample ID's and file names
  if( any( duplicated( metadata$Sample_ID ) ) ){
    print( "You have some duplicate sample IDs.")
    print( metadata$Sample_ID[ duplicated( metadata$Sample_ID ) ] )
  }
  file_dupes = metadata$raw_data_path[ duplicated( metadata$raw_data_path ) ]
  file_dupes = setdiff( file_dupes, c("") )
  if( !all( file_dupes %in% c("") ) ){
    print( "You have some duplicate files listed.")
    print( file_dupes )
  }
  
  # # Load the data; check for xist versus y chromosome genes
  all_runs = as.list(metadata$Sample_ID[metadata$file_exists_auto] ); names( all_runs ) = all_runs
  for( rep_name in names( all_runs ) ){
    all_runs[[rep_name]] = load_thymus_profiling_data( sample_ids = rep_name, test_mode = F, convert_all_to_mouse = F )[[1]]
    check_xist( raw_dge = all_runs[[rep_name]], 
                rep_name = rep_name, 
                results_path = file.path( results_path, "Xist_check" ) ) 
  }
  
  # # count transcripts, cells, umis, and reads
  basic_stats = data.frame( Reduce( rbind, lapply( all_runs, dim ) ) )
  rownames(basic_stats) = names( all_runs )
  colnames(basic_stats) = c( "transcripts", "cells" )
  basic_stats$umis_in_dge = as.numeric( lapply( all_runs, sum ) )
  basic_stats$Sample_ID = rownames( basic_stats )
  
  # # Format basic stats for seamless merging into metadata
  all_sampleids_df = read.csv( PATH_TO_METADATA, header = T, stringsAsFactors = F )[, 1, drop = F]
  rownames( all_sampleids_df ) = all_sampleids_df$Sample_ID
  basic_headers = c( "transcripts", "cells", "umis_in_dge" )
  all_sampleids_df[basic_headers] = NA
  all_sampleids_df[basic_stats$Sample_ID, basic_headers]  = basic_stats[, basic_headers]
  write.table( all_sampleids_df, file = file.path( results_path, "basics_stats.txt" ),
               quote = F, row.names = F, col.names = T, sep = "\t")
}

# # This function checks for female-specific and male-specific transcripts. 
# # It accepts a numeric matrix of raw molecule counts with genes in the rownames.
check_xist = function( raw_dge, rep_name, results_path ){
  dir.create.nice( results_path )
  print( paste0( "Checking for Xist in ", rep_name ) )
  # # try the gene as given, plus Capitalized, plus in UPPER CASE. Else return zeroes.
  friendly_gene_get = function( gene ) {
    genes_available = rownames( raw_dge )
    gene_present       = gene             %in% genes_available 
    gene_present_cap   = Capitalize(gene) %in% genes_available
    gene_present_upper = toupper(gene)    %in% genes_available
    
    if( gene_present       ){ return( raw_dge[gene,             ] ) }
    if( gene_present_cap   ){ return( raw_dge[Capitalize(gene), ] ) }
    if( gene_present_upper ){ return( raw_dge[toupper(gene),    ] ) }
    return( rep( 0, ncol( raw_dge ) ) )
  }
  
  Y_genes = read.table( file.path( PATH_TO_TABLES, "chrY_genes.txt"), stringsAsFactors = F )[[2]] %>% unique 
  Y_genes = intersect( c(Y_genes, toupper( Y_genes ) ), rownames( raw_dge ) )
  sink( file = file.path( results_path, paste0(rep_name, "_Xist_vs_Y_genes.txt" ) ) )
  {
    print( "Y_genes:" )
    print( Y_genes )
    any_y_genes_expressed = Reduce( f = "+", x = lapply( X = Y_genes, FUN = friendly_gene_get ) ) > 0
    x = table( any_y = any_y_genes_expressed, any_xist = friendly_gene_get( "XIST" ) > 0 ) 
    x %>% print
    x %>% chisq.test %>% print
  }
  sink()
  
  percentify = function( prop ) { return( 100*round( prop, 4 ) ) }
  prop_xist = mean( friendly_gene_get( "XIST" ) > 0 )
  if( prop_xist > 0 ){
    cat( paste( "Detected Xist in ", 
             percentify( prop_xist ), 
             "percent of cells in replicate", 
             rep_name, "\n" ) )
  }
}

# # This function checks for consistency across replicates as returned by `get_data_by_replicates`.
# # It plots avg expression for each gene and proportion expressing each gene.
scatterplot_replicates = function( results_path ){
  data_by_replicate = get_data_by_replicates()
  for( sample_type in names( data_by_replicate ) ){
    sample_ids = data_by_replicate[[sample_type]]
    reps = load_thymus_profiling_data( sample_ids = sample_ids, convert_all_to_mouse = F )
    all_genes = Reduce( x = lapply( reps, rownames ),f = union )
    # Initialize empty arrays
    mean_expr_by_gene = as.data.frame( matrix( 0, ncol = length( sample_ids ), nrow = length( all_genes ) ) )
    colnames( mean_expr_by_gene ) = sample_ids
    rownames( mean_expr_by_gene ) = all_genes
    prop_expr_by_gene = mean_expr_by_gene
    # Plot expression by gene both as log1p mean expression and proportion expressing.
    for( rep_id in sample_ids ){
      mean_expr_by_gene[ rownames(reps[[rep_id]]), rep_id ] = rowMeans( log1p( reps[[rep_id]]   ) )
      prop_expr_by_gene[ rownames(reps[[rep_id]]), rep_id ] = rowMeans(        reps[[rep_id]] > 0 ) 
    }
    dir.create.nice( file.path( results_path, "rep_check_total" ) )
    pdf( file.path( results_path, "rep_check_total", paste0( sample_type, ".pdf" ) ) ) 
    {
      plot_pairs( mean_expr_by_gene, main = "Total expression by gene" )
    } 
    dev.off()
    
    dir.create.nice( file.path( results_path, "rep_check_prop" ) )
    pdf( file.path( results_path, "rep_check_prop", paste0( sample_type, ".pdf" ) ) ) 
    {
      plot_pairs( prop_expr_by_gene, main = "Proportion expressing each gene" )
    } 
    dev.off()
    
  }
}

# # Another way of checking for consistency across replicates.
# # Plot a dataset's tSNE embedding, but faceted by replicate and embryonic day.
faceted_tsne = function( dge, results_path ){
  plot_df = rbind( FetchData(dge, c( "orig.ident", "eday", "tSNE_1", "tSNE_2" ) ),
                   mutate( FetchData(dge, c("orig.ident", "eday", "tSNE_1", "tSNE_2" ) ), 
                           orig.ident = "all", 
                           eday = "all") )
  old_levels = levels(plot_df$orig.ident)
  levels(plot_df$orig.ident) %<>% substring( 1, 3 ) %>% tolower %>% custom.make.unique( sep = ".5 rep ")
  write.table( cbind( old_levels, levels(plot_df$orig.ident) ) , quote = F, row.names = F )
  plot_df$rep = plot_df$orig.ident %>% substr( 11, 11 ) %>% as.character
  plot_df$rep [plot_df$rep ==""] = "1"
  p = ggplot(plot_df) + ggtitle("Cells stratified by replicate") + 
    facet_grid(rep~eday) + 
    geom_point(aes(x = tSNE_1, y = tSNE_2, colour = factor(eday)), size = 0.2) + 
    scale_color_manual(values = c(scales::hue_pal()(5), "gray"))
  fp = file.path(results_path, "tsne_replicates.pdf")
  cat( paste( "Saving faceted tSNE to", fp ) )
  ggsave( filename = fp, 
          plot = p, 
          width = 20, height = 12)
  return(p)
}
```


####Normalization and gene selection


```{r}
# We rescale everything to a certain amount of UMIs, where
# that amount is selected based on the median UMI count.
normalize_cpx_amt = function(dge, results_path, do.plot = T){

  umis_by_cell = apply( dge, 2, sum )
  assertthat::are_equal(length(umis_by_cell), length(colnames(dge)))
  magnitude = umis_by_cell %>% median %>% log10 %>% ceiling
  to.return = 10^magnitude 
  
  # # Plot results
  if( do.plot ){
      dir.create.nice( file.path( results_path, "QC" ) )
    pdf( file.path( results_path, "QC", "total_umis_by_cell.pdf"))
    {
      hist(log10(umis_by_cell), breaks = 40,
           xlab = "log10 UMI count", main = "Number of UMIs by cell")
      abline(v = to.return)
    }
    dev.off()
  }
  
  return(to.return)
}
demo1 = matrix(2000, nrow = 5, ncol = 3)
assertthat::are_equal(normalize_cpx_amt(demo1, results_path = "~/Desktop/scRNA_junk", do.plot = F), 10000)


#This function makes the arrays into Seurat objects, keeping all genes expressed in at least
#(by default) 3 cells and keeping all cells with at least 1000 genes. It reports some 
#summary figures, plotting number of genes by cell, num UMIs by cell, and number of cells by
# gene. 
seuratify_thy_data = function(raw_dge, results_path, test_mode = F, 
                              min.genes = 1000, min.cells = 3, do.plot = T){
  atat(1 < raw_dge %>% dim %>% min %>% min)

  total_desired = normalize_cpx_amt( raw_dge, results_path )
  raw_dge_norm = apply( X = raw_dge, MARGIN = 2, FUN = div_by_sum ) * total_desired
  raw_dge_norm = as( raw_dge_norm, "sparseMatrix" )
  seurat_dge = Setup( new( "seurat", raw.data = log2( 1+raw_dge_norm ) ),
                      min.cells = min.cells * (1 - test_mode), # 0 if test_mode; else 3
                      min.genes = min.genes * (1 - test_mode), # 0 if test_mode; else min.genes
                      is.expr=0, 
                      do.logNormalize = F, 
                      project = "thymus_scRNAseq", 
                      names.delim = "\\|",
                      names.field = 2 )
  
  # # Make sure the raw data is just UMI counts and update the nUMI field, which will be
  # # filled incorrectly given the above
  seurat_dge@raw.data = raw_dge[ seurat_dge@data %>% rownames, 
                                 seurat_dge@data %>% colnames ]
  seurat_dge %<>% AddMetaData( col.name = "nUMI", 
                               metadata = setNames( colSums( seurat_dge@raw.data ),
                                                    colnames( seurat_dge@raw.data ) ) )
  
  # # Plot results
  if(do.plot){
    dir.create.nice( file.path(results_path, "QC" ) )
    genes_by_cell = apply( raw_dge, 2, nnz )
    atat( length( genes_by_cell ) == ncol( raw_dge ) ) # num cells = num cols
    pdf(file.path(results_path, "QC", "total_genes_by_cell.pdf"))
    {
      hist(log10(genes_by_cell), breaks = 40,
           xlab = "log10 gene count", main = "Number of genes by cell")
      abline(v = log10( min.genes ) )
    }
    dev.off()
    
    cells_by_gene = apply( raw_dge, 1, nnz )
    atat( length( cells_by_gene ) == nrow( raw_dge )[1] ) # num genes = num rows
    pdf(file.path(results_path, "QC", "total_cells_by_gene.pdf"))
    {
      hist(log10(cells_by_gene), breaks = 40,
           xlab = "log10 cell count", main = "Number of cells by gene")
      abline(v = log10( min.cells ) )
    }
    dev.off()
    
    print( paste0("There are ", 
                  length(cells_by_gene), " genes and ", 
                  length(genes_by_cell), " cells in the raw data."))
    print( paste0( length( cells_by_gene ) - nrow( seurat_dge@data ), " genes and ", 
                   length( genes_by_cell ) - ncol( seurat_dge@data ), " cells were excluded from the Seurat object."))
    
  }

  return(seurat_dge)
}

# # Seurat preserves the raw data exactly. Sometimes that's not ideal.
# # This function helps you get rawish data that have undergone the same QC filters as the Seurat scale.data,
# # so some cells and genes are filtered out. 
# # But, the numbers are UMI counts, not logged or with any of that normalization BS.
# # This function guarantees output with `colnames(output) == dge@cell.names`.
# # 
# # Because the `@raw.data` slot was filled in wrong in some of my Seurat objects,
# # this can use `load_thymus_profiling_data` to get the raw data.
# # To toggle this behavior, set `retrieve_anew = {T,F}`.
deseuratify_raw_data = function( seurat_dge, retrieve_anew = F ){
  if( !retrieve_anew ){
    raw_dge = seurat_dge@raw.data
  } else {
    raw_dge = load_thymus_profiling_data( sample_ids = unique( FetchData(seurat_dge, "orig.ident" )[[1]]) ) %>% dge_merge_list
  }
  desired_genes = rownames( seurat_dge@scale.data )
  acceptable_genes = intersect( desired_genes, rownames( raw_dge ) )
  missing_genes    = setdiff(   desired_genes, rownames( raw_dge ) )
  raw_dge = as.matrix( raw_dge )[acceptable_genes, seurat_dge@cell.names]
  
  ##Zero-pad to ensure all genes from scale.data are present
  my_zeroes = matrix( 0, ncol = ncol( raw_dge ), nrow = length( missing_genes ) )
  rownames( my_zeroes ) = missing_genes
  colnames( my_zeroes ) = colnames( raw_dge )
  raw_dge = rbind( raw_dge, my_zeroes )
  raw_dge = raw_dge[desired_genes, ]
  
  atae(raw_dge, round( raw_dge ) )
  atae(desired_genes, rownames( raw_dge ) )
  return( raw_dge )
}

# Several different methods of variable gene selection, all based on outliers in mean-versus-sd plots.
# You can give either x and y cutoffs for these plots or the number of genes to select -- not both.
var_gene_select = function( dge, results_path, test_mode = F, 
                            prop_genes_to_select = NULL,
                            num_genes_to_select = NULL,
                            excess_var_cutoff = NULL,
                            log_expr_cutoff = NULL,
                            method = "seurat" ){
  atat( method %in% c( "seurat", "spline", "knn" ) )
  dir.create.nice( results_path )
  # # Parse input
  if( !is.null( num_genes_to_select ) & is.null( prop_genes_to_select ) ){
    prop_genes_to_select = num_genes_to_select / dim(dge@data)[1]
  }
  cutoffs_given = !(is.null(excess_var_cutoff) & is.null(log_expr_cutoff))
  prop_was_given  = ! ( is.null(prop_genes_to_select) || is.na(prop_genes_to_select) )
  assertthat::assert_that( xor(prop_was_given, cutoffs_given) )
  
  if(test_mode){
    prop_was_given = T
    prop_genes_to_select = 0.015
  }
  
  # # Need to do this to initialize @mean.var even if not going to use those cutoffs
  if(is.null(log_expr_cutoff)){log_expr_cutoff = 0}
  if(is.null(excess_var_cutoff)){excess_var_cutoff = 0}
  dge = Seurat::MeanVarPlot( dge, x.low.cutoff = log_expr_cutoff, y.cutoff = excess_var_cutoff)
  if(method == "seurat"){
    # Variable gene selection method 1 -- Seurat built-in
    dge@mean.var$avg_log_exp = dge@mean.var$data.x
    dge@mean.var$variability = dge@mean.var$data.y
    dge@mean.var$excess_variability = dge@mean.var$data.norm.y
  }
  if(method == "spline"){
    # Variable gene selection method 2 -- residuals from spline curve
    sd_log_exp  = unlist( apply( FUN = sd  , dge@data, MARGIN = 1 ) )
    avg_log_exp = unlist( apply( FUN = mean, dge@data, MARGIN = 1 ) )
    gam_data = data.frame( y = sd_log_exp, x = avg_log_exp )
    s = mgcv:::s
    mean_var_reg = mgcv::gam( data = gam_data, formula = y~s(x), family = mgcv::scat() )
    excess_variability = standardize( sd_log_exp - mean_var_reg$fitted.values )
    dge@mean.var$avg_log_exp = avg_log_exp
    dge@mean.var$variability = sd_log_exp
    dge@mean.var$excess_variability = excess_variability
  }
  if(method == "knn"){
    # # Variable gene selection method 3 -- knn-based outlier detection
    avg_log_exp = unlist( apply( FUN = mean, dge@data, MARGIN = 1 ) )
    sd_log_exp  = unlist( apply( FUN = sd  , dge@data, MARGIN = 1 ) )
    p = outlier_labeled_scatterplot( data.frame( avg_log_exp = avg_log_exp,
                                                 sd_log_exp = sd_log_exp, 
                                                 gene = rownames( dge@data ) ) )
    dge@mean.var$avg_log_exp = avg_log_exp
    dge@mean.var$variability = sd_log_exp
    temp = p$plot_env$data$dist_to_nn
    dge@mean.var$excess_variability = standardize( temp )
  }
  if(prop_was_given){
    dge@var.genes = top_n_preserve_rownames( dge@mean.var, 
                                             n = round(prop_genes_to_select * nrow( dge@mean.var ) ),
                                             wt = excess_variability ) %>% rownames
  } else {
    dge@var.genes = subset( dge@mean.var, 
                            excess_variability > excess_var_cutoff & 
                              avg_log_exp > log_expr_cutoff ) %>% rownames
  }
  dge@mean.var$included = rownames( dge@mean.var ) %in% dge@var.genes
  
  
  # # Save mean_var_plots 
  p_reg = ggplot(dge@mean.var) + ggtitle("Mean versus coef. var for all genes") +
    geom_point(aes(avg_log_exp, variability, colour = included), alpha = 0.5) + 
    geom_vline(aes(xintercept = log_expr_cutoff))
    
  ggsave(file.path(results_path, "MeanVarPlot_reg.pdf"), p_reg)
  
  p_res = ggplot(dge@mean.var) + ggtitle("Mean versus excess var across genes") +
    geom_point(aes(avg_log_exp, excess_variability, colour = included), alpha = 0.5) + 
    geom_vline(aes(xintercept = log_expr_cutoff)) + 
    geom_hline(aes(yintercept = excess_var_cutoff))
    
  ggsave(file.path(results_path, "MeanVarPlot_res.pdf"), p_res)
  
  # # Save variable genes and parameters
  vgsrp = file.path( results_path, "var_gene_select" )
  dir.create.nice( vgsrp )
  cell_markers = get_rene_markers()
  variable_cell_markers = intersect( cell_markers$marker, as.character( dge@var.genes ) )
  variable_cell_markers = c(paste0(length(variable_cell_markers), "total"), variable_cell_markers)
  text2file(file.path(vgsrp, "markers_among_variable_genes.txt"), variable_cell_markers)
  totalstring = paste(length(as.character(dge@var.genes)), "total")
  var_genes = c(totalstring, as.character(dge@var.genes))
  text2file(file.path(vgsrp, "variable_genes.txt"), var_genes)
  if( is.null( num_genes_to_select  ) ) { num_genes_to_select  = "NULL" }
  if( is.null( prop_genes_to_select ) ) { prop_genes_to_select = "NULL" }
  if( is.null( excess_var_cutoff    ) ) { excess_var_cutoff    = "NULL" }
  if( is.null( log_expr_cutoff      ) ) { log_expr_cutoff      = "NULL" }
  vsp        = c( prop_genes_to_select,   num_genes_to_select,   excess_var_cutoff,   log_expr_cutoff)
  names(vsp) = c("prop_genes_to_select", "num_genes_to_select", "excess_var_cutoff", "log_expr_cutoff")
  text2file(file.path(vgsrp, "var_gene_selection_params.txt"), collapse_by_name(vsp))
  
  return(dge)
}

save_complexity_plot = function(dge, result_path){
  dir.create.nice( file.path( result_path, "QC" ) )
  f = file.path(result_path, "QC/complexity.pdf")
  p = ggplot( data.frame( complexity = colSums( dge@raw.data > 0 ) ) ) + 
    ggtitle("Complexity") + geom_histogram(aes(x = complexity)) 
  ggsave(f, p)
  f = file.path(result_path, "QC/UMIs_by_cell.pdf")
  p = ggplot( data.frame( UMIs_by_cell = colSums( dge@raw.data ) ) ) + 
    ggtitle("UMIs per cell") + geom_histogram(aes(x = log10(UMIs_by_cell)))
  ggsave(f, p)
}

```

###Imputation

```{r}

# # Assess imputation based on the idea that observed nonzeroes, if replaced with zeroes, should be filled in
# # at higher rates than observed zeroes.
# #
# # `dge`: Seurat object
# # `results_path`--  Folder to place output in. Currently unused.
# # `threshold` -- The imputation is partly judged on different types of recovered entries 
# #   (real zeroes to nonzeroes versus simulated zeroes to nonzeroes). For some imputation methods,
# #   such as projection onto the top few PCs, everything will be nonzero, so we only count entries at or above `threshold`.
# # `imputer` -- a function to fill in zero-valued entries. Input and output  
# #   should be a numeric matrices of log normalized counts, identical to each other in dimension and dimnames. 
# # 
# # Return value is a list with entries:
# # `orig_data` -- `dge@data` gets sprinkled with simulated dropouts and fed to the imputer. 
# # `simdrop_idcs` -- the indices zeroed out to form the training set (roughly 10% of nonzeroes in each column)
# # `trainset` -- the training set for the imputer
# # `imputed_val` -- the output of the imputer
# # `recovery_curve_data` -- data for plot below
# # `recovery_curve` -- ggplot object showing recovery of simulated dropout versus recovery of non-simulated.
assess_imputation = function( dge, results_path, threshold = log1p( 1 ), imputer ){
  orig_data = dge@data
  
  # # Set 10% of nonzeroes to zero, recording which ones they were
  trainset = orig_data
  simdrop_rate = 0.1
  simdrop_idcs = as.list( colnames( trainset ) )
  names( simdrop_idcs ) = colnames( trainset )
  for( cell_i in colnames( trainset ) ) {
    nonzeroes = which( trainset[, cell_i] > 0 )
    simdrop_num = ceiling( length( nonzeroes ) * simdrop_rate )
    simdrop_idcs[[ cell_i ]] = sample( x = nonzeroes, replace = F, size = simdrop_num )
    trainset[ simdrop_idcs[[cell_i]], cell_i ] = 0
  }
  simdrop_num = Reduce( f = sum, x = lapply( simdrop_idcs, length ) )

  # # Run imputation and calculate how many entries are recovered
  imputed_vals = imputer( trainset )
  if( !identical( dimnames( imputed_vals ), dimnames( trainset ) ) ){
    warning("Grumble! Imputation method has altered row or column names. I am changing them back.")
    dimnames( imputed_vals ) = dimnames( trainset )
  }
  
  # # Compile recovered entries into a table sorted by imputed value.
  print("Categorizing recovered entries...")
  nonzero_orig = ( orig_data != 0 )
  zero_sim = ( trainset == 0 ) * ( nonzero_orig )
  nonzero_imputed = imputed_vals * ( imputed_vals >= threshold )
  recoveries_orig = summary( Matrix(  nonzero_imputed * ( !nonzero_orig ), sparse = T ) )
  recoveries_orig$type = "originally_zero"
  recoveries_sim  = summary( Matrix( nonzero_imputed * ( zero_sim ), sparse = T ) )
  recoveries_sim$type = "simulated"
  recoveries = rbind( recoveries_orig, recoveries_sim)
  recoveries = recoveries[order(recoveries$x, decreasing = T), ]
  atat( !any( duplicated( recoveries[, c("i", "j")] ) ) )

  total_rec = sum( trainset == 0 & imputed_vals >= threshold )
  simdrop_rec = 0 
  for( cell_i in colnames( trainset ) ) {
    simdrop_rec = simdrop_rec + sum( imputed_vals[ simdrop_idcs[[cell_i]], cell_i ] >= threshold )
  }
  
  atae( nrow(recoveries), total_rec )
  atae( table(recoveries$type)[["simulated"]], simdrop_rec )
  print("Computing recovery curve...")
  recovery_curve = data.frame( num_rec_allowed = seq( 1, nrow(recoveries), by = 10 ),
                               simdrop_recall = NA )
  for( ii in seq_along( recovery_curve$num_rec_allowed ) ){
    reason_for_drop = recoveries$type[ 1:recovery_curve$num_rec_allowed[ii] ]
    recovery_curve$simdrop_recall[ii] = sum( reason_for_drop == "simulated" ) / simdrop_num
  }
  p = ggplot( recovery_curve ) + geom_line(aes(x = num_rec_allowed, simdrop_recall ) )
  print( p )
  return( list( orig_data    = orig_data, 
                simdrop_idcs = simdrop_idcs, 
                trainset     = trainset,
                imputed_vals = imputed_vals,
                recovery_curve_data = recovery_curve,
                recovery_curve = p ) )
}

# # raw_dge: a numeric matrix
# # k: number of nearest neighbors.
# # Return value: raw_dge with numbers filled in.
impute_knn = function( raw_dge, k = 1, prop_nz_required = 0.2 ){
  print("Running PCA...")
  pca_obj = irlba::irlba( t( raw_dge ), 30 )
  print( "Getting nearest neighbors in projected space..." )
  knn_out = FNN::get.knn( data = pca_obj$u, k = k, algorithm=c( "cover_tree" ) )
  rownames( knn_out$nn.index ) = colnames( raw_dge )
  undo_dropout = function( x ) {
    if( mean( x!=0 ) < prop_nz_required ) { return( 0 ) }
    return( mean(x[x!=0]) )
  }
  print( "Imputing missing values..." )
  raw_dge_copy = raw_dge
  for( cell_i in colnames( raw_dge ) ) {
    to_fill = which(raw_dge[, cell_i] == 0)
    raw_dge_copy[to_fill, cell_i] = apply( X = raw_dge[to_fill, knn_out$nn.index[cell_i, ], drop = F ], MARGIN = 1, FUN = undo_dropout )
  }
  print("Done.")
  return ( raw_dge_copy )
}

```


###Removing cell-cycle variability and other gene-annotation type tasks

The function `add_cc_score` takes a Seurat object and adds cell cycle scores to the metadata (`object@data.info`). If `method = "average"`, the columns are quantitative scores for each of the five cell-cycle phases. 

- If `method == "average2"`, then only two phases are used (IG1/S and S). They are computed by adding up the expression levels of genes in five different sets; each set characterizes one phase. 
- If `method == correlation`, then each cell is annotated with its Pearson correlation with five "pseudocells". One of the pseudocells expresses all M genes equally and no others, and the others are the same but for other cell cycle phases. 
- If `method` is e.g. `pca_2`, then a PCA is computed with the union of the five gene lists as active genes, and the first two PC's are regressed out (similar to SCLVM's Gaussian model with low-rank covariance). 

```{r}
add_cc_score = function(dge, method = "average"){
  # Get cell cycle genes and expression data
  # ccDat   = read.table(file.path(PATH_TO_TABLES, "ms_cellcycleGO0007049.txt"),   sep="\t")
  macosko_cell_cycle_genes = get_macosko_cc_genes()
  phases = colnames( macosko_cell_cycle_genes )
  raw_dge = as.matrix( dge@data ) # calculate scores on log1p-scale data
  
  # # To reconcile the data with the predefined list:
  # # - get human orthologs when appropriate
  # # - get both Capital and UPPERCASE
  # # - intersect gene list with available genes
  geneset = as.list(phases); names(geneset) = phases
  for( phase in phases ){
    dge = add_maehrlab_metadata(dge, variable_to_add = "species")
    if( any( Seurat::FetchData(dge, "species")[[1]] == "human" ) ){
      human_ortho = unlist( lapply( X = macosko_cell_cycle_genes[, phase], 
                                    FUN = get_ortholog, from = "human", to = "mouse" ) )
      human_ortho = human_ortho[!is.na( human_ortho )]
    } else {
      human_ortho = c()
    }
    geneset[[phase]] = union( Capitalize( macosko_cell_cycle_genes[, phase] ), 
                     toupper(    macosko_cell_cycle_genes[, phase] ) )
    geneset[[phase]]  = union( geneset[[phase]], human_ortho )
    geneset[[phase]]  = intersect( geneset[[phase]] , rownames( raw_dge ) )
  }
  if( method == "correlation" ){
    # produce a score for each phase
    pseudocells = as.list(phases); names( pseudocells ) = phases
    for( phase in phases ){
      pseudocells[[phase]] = rownames( raw_dge ) %in% geneset[[phase]] 
    }
    scores_mat = cor( as.matrix( as.data.frame( pseudocells ) ), as.matrix( raw_dge ) )
    rownames( scores_mat ) = colnames( macosko_cell_cycle_genes )
    colnames( scores_mat ) = colnames( raw_dge )
    scores_df = data.frame(t(scores_mat))
  } else if( method == "average" ){
    # produce a score for each phase
    scores_mat = matrix( 0, nrow = length( phases ), ncol = ncol( raw_dge ) )
    rownames( scores_mat ) = colnames( macosko_cell_cycle_genes )
    colnames( scores_mat ) = colnames( raw_dge )
    for( phase in phases ){
      scores_mat[phase, ] = apply( X = raw_dge[geneset[[phase]],], MARGIN = 2, FUN = mean )
    }
    scores_df = data.frame(t(scores_mat))
  } else {
    # take the union of the cc genes and do a PCA
    method_split = down_idx(strsplit(method, split = "_"))
    method_prefix = method_split[[1]]
    method_suffix = method_split[[2]]
    atat(method_prefix == c("pca"))
    num_pc = method_suffix %>% as.numeric %>% round
    macosko_cell_cycle_genes = Reduce( union, geneset )
    macosko_cell_cycle_genes = intersect(macosko_cell_cycle_genes, rownames(raw_dge))
    dge@var.genes = macosko_cell_cycle_genes
    scores_df = Seurat::PCA(dge, pc.genes = macosko_cell_cycle_genes, pcs.store = num_pc, do.print = F)@pca.rot
    dge@var.genes = ""
    names(scores_df) = paste0("cc_pc", seq_along(scores_df))
  }  
  if( any( names( scores_df ) %in% names( dge@data.info ) ) ){
    warning("Overwriting some metadata fields in Seurat object.")
  }
  dge = Seurat::AddMetaData(dge, scores_df)
  return(list(dge = dge, cc_score_names = names(scores_df)))
}
```

###Dimension reduction

This dimension-reduction wrapper handles issues with small datasets in test mode.
 
```{r}
do_dim_red = function( dge, pc.use, ... ){
  dge = Seurat::PCA( dge, do.print = F, pcs.store = max( pc.use ) )
  if( test_mode ){
    pc.use = 1:4
    # # Solve problem downstream where t-SNE can't handle exact duplicates
    if( test_mode ) {
      dge@pca.rot = dge@pca.rot + matrix( 0.1*rnorm( prod( dim( dge@pca.rot ) ) ), nrow = nrow( dge@pca.rot ) )
    }
  } 
  dge = Seurat::RunTSNE( dge, dims.use = pc.use, ... )
  return( dge )
}
 
```

This is just a wrapper for `Seurat::VizPCA`. 

```{r}
top_genes_by_pc = function(dge, results_path, test_mode, num_pc = 30){
  if(test_mode){return()} #Doesn't work with small data. Don't care; bypassing.
  dir.create.nice(file.path(results_path, "top_genes_for_each_pc") )
  for(pc in 1:num_pc){
    pdf(file.path(results_path, "top_genes_for_each_pc", paste0("pc", pc, ".pdf")))
    VizPCA(dge, pcs.use = pc)
    dev.off()
  }
}
```

This function chooses the number of principal components. There are three options. The default uses the jackstraw, taking any principal component that is strongly associated with enough genes. Previous options, not recommended, test for associations with genes on Rene's list of cell-type markers or use a conservative take on a 2014 matrix recovery paper by Gavish and Donoho called "The Optimal Hard Threshold for Singular Values is $4/sqrt 3$". For the Gavish and Donoho approach, I take take only 20% of the singular values they recommend, because their result does not quite fit our setting as is. They optimize for plain matrix recovery, whereas we are doing other types of inference downstream that may benefit from a lower-dimensional representation of the data even if the reconstruction of the data is not optimal. For example, they assume uncorrelated noise, whereas transcript levels will show correlation structure that is not useful in detecting new subpopulations of cell types or determining which genes govern cell differentiation.

```{r}
get_pcs = function(dge, results_path, test_mode, method = "jackstraw", 
                   num_pc_to_try = 25, alpha = 1e-8, score.thresh = 1e-05){
  if(test_mode){
    pc.use = 1:4 #test mode has problems if there's not enough PC's.
    pc_pvals = NA
  } else if(method == "jackstraw"){
    dge = Seurat::JackStraw(dge, num.pc = num_pc_to_try, num.replicate = 400)
    # The overall PC p-value does not come from the jackstraw at all.
    # It is a Seurat addition, and it is buried in the Seurat jackstraw plot facet labels. 
    jsp        = Seurat::JackStrawPlot(dge, PCs = 1:num_pc_to_try, score.thresh = score.thresh)
    jsp_pretty = Seurat::JackStrawPlot(dge, PCs = 1:8,             score.thresh = score.thresh)
    ggsave(file.path(results_path, "JackStrawPlot.pdf"), jsp_pretty)
    extract_p = function(facet_label) {return(as.numeric(strsplit(as.character(facet_label), split = " ")[[1]][2]))}
    pc_pvals = purrr::map_dbl(.x = levels(jsp$data$PC.Score), .f = extract_p)
    pc.use = which(pc_pvals < alpha)
  } else if(method == "jackstraw_plus_table"){
    alpha = 0.01
    dge = Seurat::JackStraw(dge, num.pc = num_pc_to_try, num.replicate = 200, prop.freq = 0.05)
    pc_pvals = rep(NA, num_pc_to_try)
    cell_markers = get_rene_markers()
    cell_markers = Capitalize(as.character(cell_markers$marker))
    cell_markers = intersect(cell_markers, Capitalize(rownames(dge@jackStraw.empP)))
    for(pc in 1:num_pc_to_try){
      pc_pvals[pc] = length(cell_markers) * min(c(dge@jackStraw.empP[cell_markers, pc], 1), na.rm = T)
    }
    pc.use = which(pc_pvals < alpha)
  } else if(method == "gavish_donoho"){
    m = min(dim(dge@raw.data))
    n = max(dim(dge@raw.data))
    gavish_donoho_omega = function(beta)(0.56*beta^3 - 0.95*beta^2 + 1.82*beta + 1.43)
    assertthat::are_equal(gavish_donoho_omega(0.1), 1.6089, tol = 0.01)
    sv_list = dge@pca.obj[[1]]$sdev 
    pc_threshold = gavish_donoho_omega(beta = m/n) * median(sv_list) 
    num_pc = round(sum(sv_list > pc_threshold) / 5)
    pc.use = 1:num_pc
  }
  

  p = Seurat::PCElbowPlot(dge)
  ggsave(file.path(results_path, "PCA_elbow.pdf"), p)
  PC = 1:num_pc_to_try
  pc_stats = data.frame(PC, 
                        pval = pc_pvals, 
                        sdev = dge@pca.obj[[1]]$sdev[PC], 
                        used = PC %in% pc.use)
  write.table(x = pc_stats, file = file.path(results_path, "pcs_used.txt"), quote = F, sep = "\t", row.names = F)
  if(length(pc.use) < 8){warning("Less than 8 PC's selected.")}

  return(pc.use)
}
```


###Clustering
 
```{r}
# # This automatically detects and removes a biologically relevant cluster.
# # If `!do.remove`, it will return the full dataset, but with the @ident slot modified to indicate 
# # either `cluster_name` or `"rest"`.
# #
# # By default, it removes the parathyroid, but you can put in your own markers
# # (e.g. Krt5 and Krt17 to remove medullary thymic epithelial cells).
# # Some variable names still mention the parathyroid; this is an artifact of the way this software developed.
# #
# # To deal with the dropout, it runs dimension reduction and clustering not just on the markers given.
# # It also adds in the 30 most variable genes and the 30 genes whose log1p expression levels correlate
# # most strongly with the markers given.
# #
# # If you want to split cells into two ends of continuous spectrum rather than two separable clusters,
# # then set `gradient_expected = T`. This works only for very simple datasets where the continuous axis 
# # that you're interested in is the dominant source of variation, because it just chops at PC1 = 0.
# #
# # `num_data_driven_markers` : Clustering is performed using a mix of the markers given, genes that correlate with them, and variable genes.
# # The total amount of each used is `num_data_driven_markers`. 
# # 
# # As a side effect, it will save some plots to document what changes occurred. 
# # The return value is a Seurat object.
remove_cluster = function( dge, results_path, test_mode = F, markers = c( "Pth", "Chga", "Mafb" ), 
                           do.remove = T, cluster_name = "parathyroid",
                           gradient_expected = F, set_cell_type = T, num_data_driven_markers = 30 ){
  dir.create.nice( results_path )
  # # Check for, originals, UPPERCASE, and Capitalized gene names
  markers = c( markers, toupper( markers ), Capitalize( markers ) )
  markers = intersect( markers, rownames( dge@data ) )
  if( length( markers ) == 0 ){ 
    warning( "This cluster's markers are not present in the data! Returning original dataset." )
    return( dge )
  }
  
  # # Select variable genes, adding in parathyroid markers (given and also drawn from data)
  dge = var_gene_select( dge, results_path, test_mode, num_genes_to_select = num_data_driven_markers )
  data_driven_markers = get_similar_genes( dge, markers, n = num_data_driven_markers )
  dge@var.genes = union( dge@var.genes, markers )
  dge@var.genes = union( dge@var.genes, data_driven_markers )
  dge = do_dim_red( dge, results_path, test_mode, pc.use = 1:8, do.fast = T  )

  # # Run clustering and summarize mean marker expression by cluster
  if( gradient_expected ){
    dge_clustered = dge
    new_ident = rep( "PC1_small", length( dge@cell.names ) )
    new_ident[  dge_clustered@pca.rot[[1]] > 0 ] = "PC1_big"
    dge_clustered = SetIdent( dge, ident.use = new_ident)
  } else {
    dge_clustered = Seurat::FindClusters(dge, pc.use = 1:8, resolution = 1)
  }
  markers_geom_means =  aggregate.nice( x = t( dge_clustered@data[ markers, , drop = F] ) , 
                                        by = data.frame( cluster = dge_clustered@ident), 
                                        FUN = mean )
  
  # # Pick out the parathyroid using the given markers
  # # Note that this gives indices, not actual cluster names
  clusters_pth_idx = unique( apply( X = markers_geom_means, MARGIN = 2, FUN = which.max ) )
  clusters_all = rownames( markers_geom_means )
  clusters_pth     = clusters_all[  clusters_pth_idx ]
  clusters_not_pth = clusters_all[ -clusters_pth_idx ]
  if( length( clusters_pth_idx ) == 1){
    print( paste0( "All your markers are most highly expressed in cluster ", clusters_pth, "." ) ) 
  } else {
    print( "Your markers reach peak expression in different clusters. Marking all of them." )
  }
  
  # # Plot the markers and the clusters
  for( gene in markers ) {
    tsne_colored( dge_clustered , results_path, fig_name = paste0( gene, ".pdf" ), colour = gene)
  }
  tsne_colored( dge_clustered , results_path, fig_name = "clusters.pdf", colour = "ident")
  
  # # Prepare output
  if( do.remove ){
    dge_to_return = Seurat::SubsetData( dge_clustered, ident.use = clusters_not_pth )
    tsne_colored( dge_to_return, results_path, fig_name = "clusters_after_removal.pdf", colour = "ident")
  } else {
    new_ident = rep( cluster_name, length(dge_clustered@ident) ) 
    new_ident[ dge_clustered@ident %in% clusters_not_pth ] = "rest"
    dge_to_return = Seurat::SetIdent( dge_clustered, ident.use = new_ident )
    atat( length( dge_to_return@ident ) == length( dge_to_return@cell.names ) )
  }
  if( set_cell_type ){
    if( !( "cell_type" %in% names( dge_to_return@data.info ) ) ){ dge_to_return@data.info$cell_type = "rest" }
    dge_to_return@data.info$cell_type %<>% as.character
    dge_to_return@data.info$cell_type [ dge_to_return@ident == cluster_name ] = cluster_name
    atat( !any( is.na( dge_to_return@data.info$cell_type ) ) )
  }
  return( dge_to_return )
}




# # This is a test-mode-tolerant wrapper for Seurat's clustering functions.
# #   - method is either "DBSCAN" or "SNN".
# #   - granularities_as_string should be numbers separated by commas and whitespace.
# #     The number of clusterings performed is the length of the split and cleaned version of granularities_as_string.
# #     The granularity number is used as the neighborhood radius in DBSCAN or the "resolution" in SNN.
# # For more information on density-based clustering (DBSCAN), look at the KDD-96 paper by Ester, Kriegel, Sander, and Xu.
# # A density-based algorithm for discovering clusters in large spatial databases with noise.
# # For more information on the SNN-based clustering, look at (the parameter gamma in)
# # "A unified approach to mapping and clustering of bibliometric networks", 
# # Ludo Waltman, Nees Jan van Eck, and Ed C.M. Noyons

# # A postcondition of this wrapper is that cluster 1 is not a legit cluster.
# # It is either empty or the set of "rejects".
cluster_wrapper = function(dge, results_path, test_mode, 
                           pc.use = NULL, 
                           method = c("DBSCAN"),
                           granularities_as_string = "6",
                           merge_small = F){
  
  granularities = as.numeric( trimws( strsplit( granularities_as_string, split = "," )[[1]] ) )
  dir.create.nice( file.path( results_path, method ) )
  for(my_resolution in granularities){
    if(method == "DBSCAN"){
      dge = Seurat::DBClustDimension(dge, reduction.use="tsne", G.use=my_resolution, set.ident = T)
    } else {
      atat( method == "SNN" )
      dge = Seurat::FindClusters(dge, 
                                 pc.use = pc.use, 
                                 resolution = my_resolution, 
                                 print.output = F)
      ident_no_1 = dge@ident %>% as.character %>% as.numeric
      ident_no_1[ ident_no_1==1 ] = max( ident_no_1 ) + 1
      ident_no_1 = as.character( ident_no_1 )
      dge = Seurat::SetIdent(dge, ident.use = ident_no_1)
    }      
    tsne_colored( dge, file.path(results_path, method), colour = "ident",
                  fig_name = paste0("res=", my_resolution, ".pdf"))
  }
  
  

  if(test_mode & 1==length(levels(dge@ident))){
    warning("In test mode, got 1 cluster. 
            Randomly splitting into 2 clusters to facilitate debugging of differential expression code.")
    dge = Seurat::SetIdent(dge, ident.use = sample(1:2, size = length(dge@ident), replace = T) %>% as.character)
  }
  cluster_summary(dge, results_path)
  return(dge)
}

# # This records summary information for each of the clusters (currently just size). 
# # It also makes an extra copy in `named_clusters.txt` that can be hand-edited to manually name the clusters.
cluster_summary = function(dge, results_path){
  clus_summ = data.frame(table(dge@ident))
  names(clus_summ) = c("Cluster", "Num Cells")
  write.table(x = clus_summ, 
              file = file.path(results_path, "cluster_summary.txt"),
              sep = "\t", quote = F, row.names = F, col.names = T)
  clus_summ$cluster_name = "insert_name_here"
  clus_summ$color = rainbow(length(clus_summ[[1]]))
  write.table(x = clus_summ, 
              file = file.path(results_path, "named_clusters.txt"), 
              sep = "\t", quote = F, row.names = F, col.names = T)
  return( clus_summ )
}

# # This helps compare two different analyses of the same cells.
# # You put in the usual Seurat object and results path
# # plus another Seurat object that you want to compare against,
# # and names for each of them.
compare_views = function(dge, results_path, comparator_dge, dge_name, comparator_name){
  figname = paste0("embedding=", dge_name, "|colors=", comparator_name, ".pdf")
  dge = Seurat::AddMetaData( dge, metadata = comparator_dge@ident[dge@cell.names] , col.name = "other_id" )
  ggsave(file.path(results_path, figname),
         custom_feature_plot(dge, colour = "other_id"),
         width = 5.5, height = 5)
}
```


####Data exploration: differential expression and plotting


```{r}
time_series = function( dge, gene, colour = "eday", main = NULL, x = "pseudotime", col = Thanksgiving_colors ){
  atae( length( gene ), 1 )
  if( is.null(main)){ main = paste0( "Expression by ", x)}
    
  # Sanitize input -- `aes_string` chokes on a genes with hyphens (Nkx2-1)
  rownames( dge@data ) = make.names( rownames( dge@data ) )
  rownames( dge@raw.data ) = make.names( rownames( dge@raw.data ) )
  rownames( dge@scale.data ) = make.names( rownames( dge@scale.data ) )
  gene = make.names( gene )
  
  
  my_df = FetchDataZeroPad( dge, vars.all = c( x, gene, colour ) ) 
  atat( all( sapply( my_df, FUN = is.numeric)))
  s = mgcv:::s
  p = ggplot( my_df ) + ggtitle( main ) + 
    geom_smooth( aes_string( x=x, y=gene ), colour = "black",
                 method = mgcv::gam, formula = y ~ s(x),
                 method.args = list( family = mgcv::nb() )) +
    geom_point(  aes_string( x=x, y=gene, colour = colour ) ) 
  p = p + scale_y_continuous(labels = function(x) sprintf("%4.1f", x) )
  p = p + ggtitle( gene )
  if( !is.null( col ) ){ p = p + scale_color_gradientn( colours = col ) }
  return( p )
}

time_series_save = function( dge, 
                             results_path, 
                             gene,
                             x = "pseudotime",
                             types = c("pdf", "pdf_no_leg", "png_pdf_split", "pdf_no_cells"), 
                             width = 8,
                             height = 6,
                             colour = "eday",
                             ... ){
  types = tolower(types)
  # Sanitize input -- `aes_string` chokes on a genes with hyphens (Nkx2-1)
  rownames( dge@data ) = make.names( rownames( dge@data ) )
  rownames( dge@raw.data ) = make.names( rownames( dge@raw.data ) )
  rownames( dge@scale.data ) = make.names( rownames( dge@scale.data ) )
  gene = make.names( gene )
  
  
  p = time_series( dge, gene, colour = colour, x = x, ... )
  results_path = file.path( results_path, "time_series" )
  dir.create.nice( results_path )
  if( "pdf" %in% types ){
    ggsave( filename = file.path( results_path, paste0( gene, ".pdf") ),
          plot = p,
          width = width, height = height)
  } 
  if( any( c("pdf_noleg", "pdf_no_leg") %in% types ) ){
    ggsave( filename = file.path( results_path, paste0( gene, "_no_leg.pdf") ),
            plot = p + theme(legend.position="none"),
            width = width, height = height)
  }
  if( any( c( "png_pdf_split", "pdf_png_split" ) %in% types ) ){
    # PNG no axis tick labels, no axis labels, and no legend
    ggsave( filename = file.path( results_path, paste0( gene, ".png") ),
            plot = p + 
              theme(legend.position="none") +
              theme(axis.text.x  = element_blank(), 
                    axis.text.y  = element_blank()) + 
              xlab("") + ylab("") + ggtitle(""),
            width = width, height = height)
    
    # ==== PDF with no points ====
    # Copy plot and remove points
    p_no_pts = p
    p_no_pts$layers = p_no_pts$layers[1]
    # Add four points to get the right y axis and color legend
    p1 = which.max( FetchDataZeroPad( dge, gene )[[1]] )[1]
    p2 = which.min( FetchDataZeroPad( dge, gene )[[1]] )[1]
    p3 = which.max( FetchDataZeroPad( dge, colour )[[1]] )[1]
    p4 = which.min( FetchDataZeroPad( dge, colour )[[1]] )[1]
    p_no_pts = p_no_pts + geom_point( data = FetchDataZeroPad( dge, c( x, colour, gene ) )[c( p1, p2, p3, p4 ) , , drop = F],
                                      aes_string( x = x, y = gene, colour = colour ) )
    ggsave( filename = file.path( results_path, paste0( gene, "_few_pts.pdf") ),
            plot = p_no_pts ,
            width = width, height = height)
  } 
  if( "pdf_no_cells" %in% types ){
    p_no_pts = p
    p_no_pts$layers = p_no_pts$layers[1]
    ggsave( filename = file.path( results_path, paste0( gene, "_no_pts.pdf") ),
            plot = p_no_pts ,
            width = width, height = height)
  }
}

# I needed a little bit more flexibility than Seurat was giving me with the feature plots.
# Note: this used to plot the ranks of the data, but now it doesn't unless you specify `use_rank = T`.
# If you want cols.use to work for categorical variables, then it should be named with the variable's levels.
# For a blank plot (default), set `colour = NULL`.
custom_feature_plot = function(dge, colour = NULL, subset_id = NULL, axes = c("tSNE_1", "tSNE_2"),
                               alpha = 1, cols.use = blue_gray_red, use_rank = F, overplot_adjust = F, ...){
  
  # Sanitize input -- `aes_string` was choking on a gene with a hyphen (Nkx2-1)
  rownames( dge@data ) = make.names( rownames( dge@data ) )
  rownames( dge@raw.data ) = make.names( rownames( dge@raw.data ) )
  rownames( dge@scale.data ) = make.names( rownames( dge@scale.data ) )
  colour = make.names( colour )
  axes = make.names( axes )
  my_df = FetchDataZeroPad(dge, vars.all = c(axes, colour, "ident" ), use.raw = F)
  
  # # Omit some cells if user wants to omit them
  # # but keep the plotting window the same.
  if( !is.null( subset_id ) ){
    cells.use = as.character(my_df$ident) %in% as.character(subset_id)
  } else {
    cells.use = rownames(my_df)
  }
  p = ggplot() + geom_blank( aes_string( x = axes[1], y = axes[2] ), data = my_df )
  my_df = my_df[cells.use, ]
  
  # # Treat categorical variables one way and continuous one another way.
  # # For categorical, assign randomly-ordered diverging colors if none given or not enough given
  # # Convert to hexadecimal if any given as e.g. "red"
  is_categorical = (length(colour) > 0) && ( is.factor(my_df[[colour]]) | is.character(my_df[[colour]]) )
  if( overplot_adjust & is_categorical ){
    warning("Cannot adjust for overplotting with categorical variables due to color aggregation issues. 
            Continuing with `overplot_adjust=F`." )
    overplot_adjust = F
  }
  if( is_categorical ){
    is_default = ( length( cols.use ) == length( blue_gray_red ) ) && all( cols.use == blue_gray_red )
    if( is_default || length( cols.use ) < length( unique( my_df[[colour]] ) ) ){
      better_rainbow = scales::hue_pal()
      cols.use = ( my_df[[colour]] %>% unique %>% length %>% better_rainbow )
    } else if ( any( cols.use %in% colors() ) ){
      preserved_names = names(cols.use)
      cols.use = gplots::col2hex( cols.use )
      names(cols.use) = preserved_names
    }
    p = p + scale_color_manual(values = cols.use)    + scale_fill_manual(values = cols.use)
  } else { 
    if( (length( colour ) > 0) ){
      # Optional rank transformation
      if( use_rank ){
        my_df[[colour]] = rank(my_df[[colour]]) 
        p = p + labs(colour="Cell rank")
      } else {
        p = p + labs(colour="Log normalized expression")
      }
      # Set color scale by individual points, even if aggregating as in overplot_adjust
      my_limits = c(min(my_df[[colour]]), max(my_df[[colour]]))
      p = p + 
        scale_color_gradientn(colors = cols.use, limits=my_limits ) +
        scale_fill_gradientn( colors = cols.use, limits=my_limits ) 
    }
    p = p + xlab( axes[1] ) + ylab( axes[2] ) 
  }
  
 
  if( !overplot_adjust ){
    if( length( colour ) == 0 ){
      p = p + geom_point( aes_string(x = axes[1], y = axes[2]), colour = "grey25",
                          alpha = alpha, data = my_df,
                          size = 4 / log10( length( cells.use ) ) )  
    } else {
      p = p + geom_point(aes_string(x = axes[1], y = axes[2], colour = colour), 
                         alpha = alpha, data = my_df,
                         size = 4 / log10(length(cells.use))) 
      p = p + ggtitle( colour )
    }
  } else {
    if( length( colour ) == 0 ){
      p = p + geom_hex( aes_string( x = axes[1], y = axes[2], alpha = "..count.." ), fill = "grey25",
                    data = my_df )  
      p = p + ggtitle( axes_description )
    } else {
      hex_data = hexbin::hexbin(my_df)
      hex_data = data.frame( x = hex_data@xcm, 
                             y = hex_data@ycm, 
                             count = hex_data@count )
      names(hex_data)[1:2] = axes
      nearest_bin = FNN::get.knnx( query = my_df[axes], 
                                   data = hex_data[axes], 
                                   k = 1, algorithm = "cover_tree" )$nn.index %>% c
      bin_averages = aggregate.nice( my_df[[colour]], by = nearest_bin, FUN = mean )[, 1]
      hex_data[names(bin_averages), colour] = bin_averages
      p = p + geom_point( aes_string(x = axes[1], y = axes[2], size = "count", colour = colour ),
                          data = hex_data )
      hex_data = subset( hex_data, count > 20 )
      p = p + ggtitle( colour )
    }
  }
  return(p)
}
```


```{r}
#' Save feature plots. 
#'
#' @param dge Seurat object
#' @param results_path
#' @param colour A gene or type of metadata. Numeric zeroes plotted if `!is.element( colour, AvailableData( dge ) )`.
#' @param fig_name Figure gets named <fig_name>.pdf or <fig_name>.png or similar. If you put a name ending in ".png" or ".pdf", the extension is stripped off.
#' @param axes Character vector of length 2. Name of numeric variables available from `FetchData`.
#' @param axes_description Character. Used in file paths, so no spaces please.
#' @param alpha Numeric of length 1 between 0 and 1. Point transparency.
#' @param height Passed to ggsave.
#' @param width Passed to ggsave, but when you ask for a legend, it gets stretched a bit to make up for lost horizontal space.
#' @param types Atomic character vector; can be longer than 1 element. If contains "PDF", you get a PDF back. If "PDF_no_leg", you get a PDF with no legend. If "PNG_PDF_split", you get back the points and bare axes in a PNG, plus text-containing elements in a PDF with no points. By default, does all three. Matching is not case sensitive.
#' @param ... Additional arguments passed to `custom_feature_plot`.
#'
tsne_colored = function(dge, results_path, colour = NULL, fig_name = NULL,
                        axes = c("tSNE_1", "tSNE_2"), axes_description = "TSNE", 
                        alpha = 1, height = 7, width = 8, 
                        types = c("PDF", "PDF_no_leg", "PNG_PDF_split"), ... ){
  
  # Sanitize input -- `aes_string` was choking on a gene with a hyphen (Nkx2-1)
  rownames( dge@data ) = make.names( rownames( dge@data ) )
  rownames( dge@raw.data ) = make.names( rownames( dge@raw.data ) )
  rownames( dge@scale.data ) = make.names( rownames( dge@scale.data ) )
  colour = make.names( colour )
  axes == make.names( axes )
  
  # More input cleaning
  types = tolower(types)
  if( is.null( fig_name ) ){ fig_name = colour }
  fig_name %<>% strip_suffix( ".pdf" )
  fig_name %<>% strip_suffix( ".png" )
  
  # Get plot
  p = custom_feature_plot(dge = dge, colour = colour, axes = axes, alpha = alpha, ...)
  
  # Save plots
  stretch = (1 + 0.025*max( nchar(colour), 0))
  dir.create.nice( file.path( results_path, axes_description ) )
  if( "pdf" %in% types ){
    ggsave( filename = file.path( results_path, axes_description, paste0(fig_name, ".pdf") ),
          plot = p,
          width = width*stretch, height = height)
  } 
  if( any( c("pdf_noleg", "pdf_no_leg") %in% types ) ){
    ggsave( filename = file.path( results_path, axes_description, paste0(fig_name, "_no_leg.pdf") ),
            plot = p + theme(legend.position="none"),
            width = width, height = height)
  }
  if( any( c( "png_pdf_split", "pdf_png_split" ) %in% types ) ){
    # PNG no axis tick labels, no axis labels, and no legend
    ggsave( filename = file.path( results_path, axes_description, paste0(fig_name, ".png") ),
            plot = p + 
              theme(legend.position="none") +
              theme(axis.text.x  = element_blank(), 
                    axis.text.y  = element_blank()) + 
              xlab("") + ylab("") + ggtitle(""),
            width = width*stretch, height = height)
    
    # ==== PDF with no points ====
    # Copy plot and remove points
    p_no_pts = p
    p_no_pts$layers = p_no_pts$layers[1]
    # Add two points to get the right color legend 
    if(length(colour)!=0){
      max_idx = which.max( FetchDataZeroPad(dge, colour)[[1]] )[1]
      min_idx = which.min( FetchDataZeroPad(dge, colour)[[1]] )[1]
      p_no_pts = p_no_pts + geom_point( data = FetchDataZeroPad( dge, c( axes, colour ) )[c( max_idx, min_idx ) , ],
                                        aes_string( x = axes[[1]], y = axes[[2]], colour = colour ) )
    }
    ggsave( filename = file.path( results_path, axes_description, paste0(fig_name, "_no_pts.pdf") ),
            plot = p_no_pts ,
            width = width, height = height)
  }

}
```

```{r}

# # Given a dge with fields filled in for PCA rot or tSNE rot, 
# # this saves a bunch of summary data: eday, clusters, replicates, nUMI, and pseudotime if available.
misc_summary_info = function(dge, results_path, clusters_with_names = NULL,
                             axes = c("tSNE_1", "tSNE_2"), axes_description = "TSNE", alpha = 1,
                             ident.use = "eday" ){
  results_path = file.path( results_path, "summaries" )
  
  # # Checks automatically whether the colour variable is available
  fplot = function( fig_name, colour, ... ){
    if( length( colour ) == 0 || colour %in% c( names( dge@data.info ), "ident" ) ){
      tsne_colored( dge = dge, results_path,
                    fig_name = fig_name, colour = colour, 
                    axes = axes, axes_description = axes_description, alpha = alpha, ...)
    } else {
      print( paste0( "Skipping summary of ", colour, " because it's not available." ) )
    }
  }

  # # lots of custom-color feature plots
  replicate_colors = c( "orange", "blue", "red", "brown", "pink", "black", "green", "purple", "lightcoral" )
  fplot( "replicates.pdf", "rep", cols.use = gplots::col2hex( replicate_colors ) )
  fplot( "cell_type.pdf", "cell_type" )
  fplot( fig_name = "plain_gray.pdf", colour = NULL )
  fplot( "clusters.pdf", "ident" )
  fplot( "samples.pdf", "orig.ident" )
  fplot( "nGenes.pdf", "nGenes" )
  fplot( "branch.pdf"  , "branch" )
  fplot( "day.pdf", "eday", cols.use = Thanksgiving_colors )
  if( "human_labeled_clusters" %in% names( dge@data.info ) && !is.null( clusters_with_names ) ){
    cu = clusters_with_names$color
    names(cu) = clusters_with_names$cluster_name %>% as.character
    fplot( "human_labeled_clusters.pdf", "human_labeled_clusters" )
  }
  
  # # add branch ID and PT from Monocle or similar
  if( "pseudotime" %in% names( dge@data.info ) ) {
    ggsave( filename = file.path( results_path, "pseudotime_by_eday_box.pdf"),
            plot = ggplot( dge@data.info, aes( y = pseudotime, x = factor( eday ) ) ) + geom_boxplot() )
    dge = AddMetaData( dge, col.name = "DI", 
                       metadata = FetchData(dge, "pseudotime") %>% vectorize_preserving_rownames )
    ggsave( filename = file.path( results_path, "di_by_eday_density.pdf"),
            plot = ggplot( dge@data.info ) + ggtitle( "DI by day" ) + 
            geom_density( aes( x = DI, fill = eday, group = eday ), alpha = 0.4 ) +
              scale_fill_gradientn( colours = Thanksgiving_colors ) )
    fplot( fig_name = "pseudotime.pdf" , colour = "pseudotime" )
    fplot( fig_name = "DI.pdf" , colour = "DI", cols.use = Thanksgiving_colors )
  }
}


#' Save a heatmap or time-series plot for every gene in `gene_list`.
#'
#' @param dge Seurat object with available t-SNE coords (or whatever's in `axes`) 
#' @param results_path: where to save the resulting plots
#' @param top_genes: deprecated; do not use
#' @param by_cluster: deprecated; do not use
#' @param gene_list: character vector consisting of gene names
#' @param gene_list_name: used in file paths so that you can call this function again with different `gene_list_name`
#    but the same results_path and it won't overwrite.
#' @param axes: any pair of numeric variables retrievable via FetchData. Defaults to `c("tSNE_1", "tSNE_2")`.
#' @param axes_description: used in file paths so that you can call this function again with different `axes_description` but the same `results_path` and it won't overwrite.
#' @param time_series: Uses `time_series` internally instead of `custom_feature_plot`. Changes defaults for
#' `axes` and `axes_description`.
#' @param alpha Transparency of points
#' @param ... Additional parameters are passed to `custom_feature_plot` or `time_series`
save_feature_plots = function( dge, results_path, 
                               top_genes = NULL, 
                               by_cluster = NULL,
                               gene_list = NULL, 
                               gene_list_name = NULL, 
                               axes = NULL,
                               axes_description = NULL,
                               do_time_series = F,
                               alpha = 1, ... ){
  # # Adjust defaults sensibly
  if( do_time_series ){
    if( is.null( axes            ) )  { axes             = "pseudotime" }
    if( is.null( axes_description ) ) { axes_description = "pseudotime" }
  } else {
    if( is.null( axes             ) ) { axes = c( "tSNE_1", "tSNE_2" ) }
    if( is.null( axes_description ) ) { axes_description = "TSNE" }
  }
  
  # # Defaults to rene's markers if gene_list not given
  # # If gene_list is not given, gene_list_name is replaced with "rene_picks"
  # # gene_list_name defaults to "unknown" if only gene_list_name not given
  if( is.null( gene_list ) ){
    gene_list = get_rene_markers()$marker
    if( !is.null( gene_list_name ) ){
      warning("Overwriting gene_list_name argument with 'rene_picks' since gene_list was not given.")
    }
    gene_list_name = "rene_picks"
  } else if(is.null(gene_list_name)){
    warning("Please fill in the gene_list_name argument. Defaulting to 'unknown'.")
    gene_list_name = "unknown"
  }
  
  if(!is.null(top_genes) || !is.null(by_cluster)){
    warning( paste ( "`top_genes` and `by_cluster` arguments have been deprecated.",
                     "If you want plots of cluster markers, use the new arg `gene_list_name`." ) )
  }
  
  # # Put all feature plots in one PDF
  no_data = c()
  feature_plots_path = file.path(results_path, "feature_plots", gene_list_name)
  dir.create.nice( feature_plots_path )
  dir.create.nice( file.path( feature_plots_path ) )
   
  gene_list = as.character( gene_list )
  for( gene_name in gene_list ){
    if( !do_time_series ){
      tsne_colored( dge, results_path = feature_plots_path, colour = gene_name, 
                    axes = axes, axes_description = axes_description, alpha = alpha, ... )
    } else {
      time_series_save( dge, results_path = feature_plots_path, gene = gene_name, ... )
    }
  } 
  cat( "Plots saved to", file.path( feature_plots_path ), "\n" )
}


# # This function streamlines the processing of marker genes for the special case of a head-to-head comparison. 
# # It obtains and saves: 
# #  - statistics on potential marker genes
# #  - top-ranking marker genes as measured by AUC
# #  - top-ranking marker genes cross-referenced with mouse transcription factors and poised genes
# #  - feature plots for top genes
# #
# # theme: a short string describing the analysis being done.
# # note: a longer string describing the analysis being done.
# # ident.1: should be %in% dge@ident
# # ident.2: should be %in% dge@ident and different from `ident.1`.
# # auc_cutoff: genes get included in `top_markers` if their AUC is above this or below one minus this.
# # ... : additional arguments to pass to `save_feature_plots`.
head_to_head_document = function( dge, results_path, theme, note, ident.1, ident.2, auc_cutoff, ...){
  dir.create.nice( file.path(results_path, theme ) )
  text2file(file.path(results_path, theme, "explanation.txt"), note)
  
  de_genes = FindMarkers(dge, ident.1 = ident.1, ident.2 = ident.2, test.use = "roc")
  de_genes = de_genes[ order(de_genes$myAUC, decreasing = T), ]
  de_genes$gene = rownames( de_genes )
  de_genes$cluster = ident.1
  de_genes$cluster[ de_genes$avg_diff < 0 ] = ident.2
  markers_top     = subset( de_genes, myAUC < (1-auc_cutoff) | auc_cutoff < myAUC  ) 
  markers_tf      = subset( markers_top, gene %in% get_mouse_tfs() )
  markers_poised  = subset( markers_top, gene %in% get_poised_genes() )
  write.table(x = de_genes,       file = file.path(results_path, theme, "markers.txt") )
  write.table(x = markers_top,    file = file.path(results_path, theme, "markers_top.txt") )
  write.table(x = markers_tf,     file = file.path(results_path, theme, "markers_tf.txt") )
  write.table(x = markers_poised, file = file.path(results_path, theme, "markers_poised.txt") )
  
  save_feature_plots( dge, 
                      results_path = file.path(results_path, theme), 
                      gene_list = markers_top$gene, 
                      gene_list_name = theme, ... )
  return()
}

# # Find genes with expression patterns similar to the genes you've specified.
# #
# # `dge` : a Seurat object with field `@scale.data` filled in.
# # `markers`: a character vector; giving gene names.
# # `n`: integer; number of results to return.
# # `anticorr` : allow negatively correlated genes; defaults to `FALSE`.
# # Given a Seurat object and a list of gene names, this function returns genes 
# # that are strongly correlated with those markers. 
# # Return value: character vector.
get_similar_genes = function( dge, markers, n, anticorr = F ){
  data.use = dge@scale.data
  if(!all(markers %in% rownames(data.use))){ 
    warning("Some of your markers have no data available. Trying Various CASE Changes.")
    markers = unique( c( markers, toupper(markers), Capitalize( markers ) ) )
  }
  markers = intersect(markers,
                      rownames( data.use) ) 
  correlation = rowSums( data.use %*% t( data.use[markers, , drop = F]) ) 
  correlation = correlation[ setdiff( names( correlation ), markers ) ]
  if( anticorr ){
    similar_genes = names( sort( abs( correlation ), decreasing = T )[ 1:n ] )
  } else {
    similar_genes = names( sort( correlation, decreasing = T )[ 1:n ] )
  }
  return( similar_genes )  
}
```
####Pathway analysis

```{r}
do_enrichr = function( results_path, geneset, geneset_name, 
                       desired_db = c( "KEGG_2016", 
                                       "WikiPathways_2016",
                                       "Reactome_2016",
                                       "BioCarta_2016",
                                       "Panther_2016",
                                       "NCI-Nature_2016", 
                                       "GO_Biological_Process_2015" ),
                       N_ANNOT_PER_DB = 2 ){
  dir.create.nice( results_path )
  
  # # Get enrichr results and parse them
  output_table = enrichR::enrichGeneList( gene.list = geneset, databases = desired_db ) 
  output_table %<>% group_by( database ) %>% top_n( wt = -pval, n = N_ANNOT_PER_DB) %>% as.data.frame
  output_table$pval = NULL
  output_table %<>% mutate( log10_qval = round( log10( qval ), 1 ) )
  output_table$qval = NULL
  output_table = output_table[c(1,2,4,3)]
  # # Save raw table
  write.table( x = output_table, 
               file = file.path( results_path, paste0("annot_", geneset_name, "_raw.txt" ) ),
               sep = "\t", quote = F, row.names = T, col.names = T )
 
  write.table( x = geneset, 
               file = file.path( results_path, paste0("annot_", geneset_name, "_annotated_genes.txt" ) ),
               sep = "\t", quote = F, row.names = F, col.names = F )
  # # Don't include genes in pretty version
  output_table$genes  = NULL

  
  # # Set up color palette and print to file
  n_colors = length(unique(output_table$database)); 
  color_idx = output_table$database %>% factor(levels = desired_db, ordered = T) %>% as.integer
  my_cols = scales::hue_pal()(n_colors)[ color_idx ]
  theme_color_db = ttheme_minimal( core=list( bg_params = list( fill = my_cols ) ) )
  ggsave( tableGrob( d = output_table, theme = theme_color_db ), 
          file = file.path( results_path, paste0("annot_", geneset_name, "_color=database.pdf")  ), 
          width = 15, height = N_ANNOT_PER_DB*length(desired_db) / 3, limitsize = F )
}
```

####Heatmapping

```{r}

# Helper function. Check if a list of markers is compatible with a given Seurat object 
# so that genes and cluster assignments are present in both `marker_info` and
# the Seurat object `dge`.
# If `desired_cluster_order` is given, `are_compatible` checks that it is 
# free of duplicates and it is a superset of the identity values occurring in other inputs.
are_compatible = function( dge, marker_info, ident.use ){
  atat( all( c("gene", "cluster") %in% names( marker_info ) ) )
  factor_flag = FALSE
  for( i in seq_along( marker_info ) ) {
    factor_flag = factor_flag || is.factor( marker_info[[i]] )
    marker_info[[i]] = as.character( marker_info[[i]] )
  }
  if( factor_flag ){ warning( "Factor columns detected in marker_info." ) }
  
  dge_ident =  as.character( FetchData( dge, ident.use )[[1]] )
  gene_compat  = all( marker_info$gene    %in% rownames( dge@data ) )
  ident_compat = all( marker_info$cluster %in% dge_ident )
  if( !gene_compat ){
    warning("marker_info$cluster has ID's not available in Seurat object")
  }
  if( !ident_compat ){
    warning("marker_info$gene has genes not available in Seurat object")
  }
  return( gene_compat && ident_compat )
}

# Check if a list of cell types is good to be used to order genes and cells in a heatmap.
# - no duplicates
# - as a set, equal to union of table cluster labels and dge cluster labels
# - no missing values
fix_cluster_order = function(  dge, marker_info, ident.use, desired_cluster_order = NULL ){
  if( is.null( desired_cluster_order ) ){
    warning( "No cluster order specified. Ordering clusters stupidly." )
    desired_cluster_order = union( Seurat::FetchData( dge, ident.use )[[1]], 
                                   marker_info$cluster )
  }
  atae( typeof( desired_cluster_order ), "character" )
  all_celltypes = union( Seurat::FetchData(dge, ident.use)[[1]], marker_info$cluster )
  missing = setdiff( all_celltypes, desired_cluster_order)
  extra   = setdiff( desired_cluster_order, all_celltypes)
  if( length( missing ) > 0 ){
    warning("Adding missing cell types to `desired_cluster_order` from Seurat object or marker table.")
    desired_cluster_order = c(desired_cluster_order, missing)
  }
  if( length( extra ) > 0 ){
    warning("Removing extraneous cell types from `desired_cluster_order`.")
    desired_cluster_order = intersect( desired_cluster_order, all_celltypes )
  }
  if( anyDuplicated( desired_cluster_order ) ){
    warning("Omitting duplicates in `desired_cluster_order`.")
    desired_cluster_order = unique( desired_cluster_order )
  }
  atat(  !any( is.na( desired_cluster_order ) ) )
  return( desired_cluster_order )
}


# This function returns a new ordering of the cells (a list of cell names).
# Cells will be ordered first by cluster and then (within clusters) by the sum of 
# the expression levels of that cluster's markers.
# `marker_info` should be a dataframe containing variables `gene` and `cluster`.
# `dge` should be a Seurat object.
# `ident.use` tells you what variable to pull from the Seurat object.
# The first three args should be compatible according to `are_compatible`.
# Also, `desired_cluster_order` should be a superset of `Seurat::FetchData(dge, vars.all = ident.use)[[1]]`.
optimize_cell_order = function(dge, marker_info, ident.use, desired_cluster_order ){
  # # Check inputs
  atat( are_compatible( dge, marker_info, ident.use ) )
  desired_cluster_order = fix_cluster_order( dge, marker_info, ident.use, desired_cluster_order )
  # # Set up vector: gets sorted on values, but the item of interest is the resulting ordering of the names.
  cluster_labels_orig = factor( as.character( Seurat::FetchData(dge, ident.use)[[1]]), 
                                levels = desired_cluster_order, 
                                ordered = T ) 
  names( cluster_labels_orig ) = dge@cell.names
  
  # Put clusters together
  # clcibcn means cluster_labels_contiguous_indexed_by_cell_names
  clcibcn = sort(cluster_labels_orig)

  # Rearrange each cluster by its top markers
  for(cluster_id in unique( marker_info$cluster )){
    this_cluster_idx =       ( clcibcn == cluster_id )
    this_cluster_cells = names(clcibcn)[ this_cluster_idx ]
    markers.use = marker_info$gene[marker_info$cluster == cluster_id]
    if( sum( marker_info$cluster == cluster_id ) == 0){ next } # skip reordering if no markers available
    new_cell_order = Seurat::FetchData(dge, 
                                       vars.all = markers.use, 
                                       cells.use = this_cluster_cells) %>% 
      rowSums %>% sort %>% names
    clcibcn[this_cluster_idx] = clcibcn[new_cell_order]
  }
  
  # Make sure clusters are still together
  assertthat::are_equal(sort(clcibcn), clcibcn)
  return(names(clcibcn))
}
```


```{r}

# Set up a sparse axis with few labels for many x/y values
# Not an ideal interface (sorry!): for eight observations in groups of 5 and 3, 
# the `labels` input must be like c("", "", "lab1", "", "", "", "lab2", "").
sparse_axis = function(labels, side, ...){
  for( i in seq_along( labels ) ){ 
    if( "" != labels[i] )
      axis(side = side, at = i / length( labels ), labels = labels[i], ... ) 
    par(las = 2)
  }
}

# This function saves a big fucking PDF file to `<results_path>/<main>.pdf` containing 
# a heatmap of gene expression levels.
# Each column is a cell and each row is a gene. Each gene is rescaled so that its peak expression is 1.
# This facilitates comparison within genes and across cells, though it's bad for comparison across genes.
# 
# `marker_info` should be a dataframe containing variables `gene` and `cluster`.
# `dge` should be a Seurat object.
# `results_path` should be a character such that `dir.exists( results_path )`.
# `desired_cluster_order`: a superset of 
# if `do_key`, a color legend gets added.
# `cs` is a vector of color names.
# If `dendrogram`, a cell dendrogram is added. It is constrained to be compatible with `dge@ident`. The 
# implementation changes considerably; this feature is under development and other inputs -- especially `desired_cluster_order` -- may not work properly.
# If `test_mode`, use only 100 genes and 100 cells.
save_heatmap = function( dge, results_path, marker_info, 
                         desired_cluster_order = NULL,
                         main = "heatmap",
                         ident.use = "ident",
                         do_key = F,  
                         cs = blue_yellow, 
                         dendrogram = F, 
                         test_mode = F ){
  if( do_key ){warning("Sorry, do_key is not implemented right now.")}

  # # Check inputs
  atat( are_compatible( dge, marker_info, ident.use ) )
  desired_cluster_order = fix_cluster_order( dge, marker_info, ident.use, desired_cluster_order )
  
  # reorder genes and set sparse column labels
  marker_info$cluster = factor( as.character(marker_info$cluster), 
                                   levels = desired_cluster_order, 
                                   ordered = T)
  marker_info = marker_info[order(marker_info$cluster), ]
  gene_labels = rep("", length( marker_info$cluster ))
  tmc = table( marker_info$cluster )
  tmc = tmc[tmc > 0] # Don't want to include labels such as "doublets" if there are no corresponding markers.
  non_blanks = round(cumsum(tmc) - tmc/2)+1
  gene_labels[non_blanks] = names(tmc) 

  # reorder cells and set sparse row labels
  cell_order = optimize_cell_order( dge, marker_info = marker_info, ident.use = ident.use, 
                                    desired_cluster_order = desired_cluster_order )
  cell_clusters = Seurat::FetchData(dge, ident.use)[cell_order, 1] %>% as.character
  cell_labels = rep("",length(cell_clusters))
  tcc = table(cell_clusters)[desired_cluster_order]
  tcc = tcc[tcc > 0] # Don't want to include labels such as "doublets" if there are no corresponding cells.
  non_blanks = round(cumsum(tcc)-tcc/2)+1
  cell_labels[non_blanks] = names(tcc) 
  
  # sweep out max expression level and set colorscale
  norm_expr = t( apply(X = dge@data[marker_info$gene, cell_order], FUN = div_by_max, MARGIN = 1) )

  if( test_mode ){
    rand_idx = sample(1:min(dim(norm_expr)), size = 100, replace = F) %>% sort
    cell_labels = cell_labels[rand_idx]
    gene_labels = gene_labels[rand_idx]
    norm_expr = norm_expr[rand_idx, rand_idx]
    marker_info = marker_info[rand_idx, ]
  }
  
  num_breaks = length(cs) + 1
  max_num_clust = max( length(unique(marker_info$cluster)), length(unique(dge@ident)) )
  # # diverging
  categ_colors = colorspace::rainbow_hcl(n = max_num_clust)
  # # alternating gray
  # categ_colors = colorspace::sequential_hcl(n = 2, h = 0, c = 0, l = c(20, 80))[1 + mod(0:max_num_clust, 2)] 
  # time to rock and roll!
  names(categ_colors) = desired_cluster_order

  print("Making heatmap...")
  fname = paste0( main, ".pdf" )
  pdf( file.path( results_path, fname ) )
  {
    if(!dendrogram){
      image( z = t(norm_expr), col = cs, xaxt = "n", yaxt = "n", main = main, xlab = "Cells", ylab = "Genes" ) 
      sparse_axis( labels = gene_labels, side = 2, tick = F )
      sparse_axis( labels = cell_labels, side = 1, tick = F )
      # # fields::image.plot is a good basic setup with color key, if you can ever get the damn thing working.
    } else {
      gplots::heatmap.2( norm_expr, 
                         Rowv = F, 
                         Colv = T, 
                         dendrogram = "column",
                         symm = F, 
                         scale = "none", 
                         col = blue_yellow,
                         trace = "none",
                         labCol = cell_labels, xlab = "Cells", 
                         labRow = gene_labels, ylab = "Genes",
                         ColSideColors = categ_colors[ dge@ident[ colnames( norm_expr ) ] %>% as.character ] ,
                         RowSideColors = categ_colors[ marker_info$cluster %>% as.character ] )
    }
  }
  dev.off()
  print( paste0( "Heatmap saved as ", fname) ) 
}
```

```{r}
# # Makes a heatmap with one column for each cluster in `unique( Seurat::FetchData(dge, ident.use)[[1]])` and 
# # one row for every gene in `genes_in_order`. 
# # If the cluster expression values are stored in `x`, then `aggregator(x)` gets (normalized and) plotted.
# # Optional parameter `desired_cluster_order` gets coerced to character. Should be a permutation of 
# # `unique(Seurat::FetchData(dge, ident.use))`, though elements may be omitted.
make_heatmap_for_table = function( dge, genes_in_order, 
                                   desired_cluster_order = NULL, 
                                   ident.use = "ident",
                                   labels = NULL, 
                                   aggregator = mean, 
                                   normalize = "row", 
                                   norm_fun = div_by_max,
                                   main = "Genes aggregated by cluster" ){

  # # Set up simple ident variable
  if(is.null(desired_cluster_order)){
    warning("No cell-type ordering given. Using arbitrary ordering.")
    desired_cluster_order = list(FetchData(dge, ident.use)[1, 1])
  }
  marker_info = data.frame( gene = genes_in_order, cluster = desired_cluster_order[[1]] )
  desired_cluster_order = fix_cluster_order( dge, marker_info, ident.use, desired_cluster_order )
  ident = FetchData(dge, ident.use) %>% 
    vectorize_preserving_rownames %>% 
    factor(levels = desired_cluster_order, ordered = T)
  ident = sort(ident)
  
  # # Sanitize input -- characters for genes, and no duplicate genes.
  genes_in_order = as.character( genes_in_order )
  if( anyDuplicated( genes_in_order ) ){
    warning( "Sorry, can't handle duplicate genes. Removing them." )
    genes_in_order = genes_in_order[ !duplicated( genes_in_order )]
  }
  if( !all( genes_in_order %in% AvailableData(dge) ) ){
    warning( "Some of those markers are not available." )
    genes_in_order = intersect( genes_in_order, AvailableData(dge) )
  }
  
  # # Get cluster mean expression for each gene and row normalize
  logscale_expression = Seurat::FetchData(dge, vars.all = genes_in_order)[names( ident ), ]
  expression_by_cluster = aggregate.nice( x = logscale_expression, by = list( ident ), FUN = aggregator )
  if( normalize == "row" ){
    expression_by_cluster = apply(X = expression_by_cluster, FUN = norm_fun, MARGIN = 2 ) %>% t
  } else if( normalize == "column" ){
    expression_by_cluster = apply(X = expression_by_cluster, FUN = norm_fun, MARGIN = 1 ) 
  } else if( normalize != "none"){
    warning('normalize should be one of "row", "column", or "none". Performing row normalization.')
    normalize = "row"
  } else {
    expression_by_cluster = t(expression_by_cluster)
  }
  
  # # Form matrix in shape of heatmap and then melt into ggplot
  plot_df_wide = cbind( as.data.frame( expression_by_cluster ) , gene = rownames(expression_by_cluster))
  plot_df_wide$y = 1:nrow(plot_df_wide)
  plot_df_long = reshape2::melt( plot_df_wide, 
                                 id.vars = c("gene", "y"), 
                                 value.name = "RelLogExpr")

  plot_df_long$RelLogExpr = plot_df_long$value
  plot_df_long$Cluster = factor( as.character( plot_df_long$variable ),
                                 levels = desired_cluster_order )
  plot_df_long$gene = factor( as.character( plot_df_long$gene ),
                                 levels = genes_in_order )

  p = ggplot( plot_df_long ) + ggtitle( main ) +
    geom_tile( aes(x = Cluster, y = gene, fill = RelLogExpr ) )
  p = p + theme(axis.text.x = element_text(angle = 45, hjust = 1))

  if( is.null( labels ) ){ 
    if( length( genes_in_order ) < 30 ){
      labels = "regular"
    } else if ( length( genes_in_order ) < 80 ){
      labels = "stagger"
    } else {
      labels = "none"
    }
  }
  
  if( labels=="none"){
    p = p + theme(axis.ticks.y=element_blank(), axis.text.y = element_blank()) 
  }
  # # Add staggered labels with invisible one farther out to make room
  if( labels == "stagger" ){
    p = p + theme(axis.ticks.y=element_blank(), axis.text.y = element_blank())
    plot_df_long$stagger_pos = rep( c(-0.75, 0), length.out = nrow( plot_df_long ) )
    invisible_label_row = plot_df_long[1, ]
    invisible_label_row$gene = ""
    invisible_label_row$stagger_pos = -1.5
    invisible_label_row$y = 1
    plot_df_long = rbind(plot_df_long, invisible_label_row)
    p = p + geom_text(data = plot_df_long, aes(x = stagger_pos, y = y, label = gene ))
  }
  print(p)
  
  return( p )
}

```


```{r}

# # Makes a heatmap of cell-cycle scores by `ident.use`.
do_cc_heatmap = function( dge, results_path, ident.use = "eday" ){
  available = ( ident.use %in% names( dge@data.info ) )
  if( is.null( ident.use ) ){ return( ) }
  if(           !available ){ return( ) }
  dir.create.nice( results_path )

  # # Heatmap of cell cycle by eday
  dge = add_cc_score( dge )$dge
  
  cc_scores = dge@data.info[, CC_PHASES] 
  # standardize = function(x)( return ( ( x - median( x ) )  / IQR( x ) ) ) 
  # cc_scores = apply( X = cc_scores, MARGIN = 2, FUN = standardize)
  phases_by_day = aggregate.nice( cc_scores, by = dge@data.info[, ident.use], FUN = mean)
  phases_by_day = as.data.frame(phases_by_day)
  phases_by_day = data.frame(  apply( X = phases_by_day[, CC_PHASES], MARGIN = 2, FUN = div_by_max ) ) 
  phases_by_day[[ident.use]] = rownames( phases_by_day )

  # Save plot normalized by by cc phase
  cc_long = melt( phases_by_day, variable_name = "cc_phase", id.vars = ident.use )
  cc_long$score = cc_long$value
  p = ggplot( data = cc_long ) + 
    geom_tile( aes_string( x = ident.use, y = "cc_phase", fill = "score" ) )
  ggsave(filename = file.path( results_path, "cell_cycle.pdf" ), plot = p)

  return( p )
}

```

###Pseudotime analysis


####Pseudotime analysis


```{r}
#' Run Monocle (or simpler PCA-based pseudotime) on a Seurat object, transferring input and output automatically to and from a CellDataSet object.
#'
#' @param dge Seurat object
#' @param results_path Where to save tables and plots
#' @param earliest_day Numeric. Used to root the Monocle trajectory; gets compared to `FetchData(dge, "eday")[[1]]` via `==`.
#' @param mp Passed to call_monocle_on_seurat as monocle_params arg.
#' @param ... Additional parameters passed to call_monocle_on_seurat.
#'
#' @details This function will look for interesting genes of different types and save a ton of plots.
#' The statistical inference is outsourced to Monocle via call_monocle_on_seurat (or Seurat via pc_as_pt).
#'
master_pt = function( dge, results_path, method = "monocle", earliest_day = NULL,
                      mp = list( reset_var_genes       = T, 
                                 log_scale_expr_thresh = 0.1,
                                 excess_disp           = 1,
                                 num_mature_types      = NULL,
                                 reduction_method      = "DDRTree" ), ...  ){
  # # Run monocle & transfer results
  dir.create.nice( results_path )
  if( method=="PCA" ){
    dge = pc_as_pt( dge )$dge
  } else if (method=="DPT") {
    dm = destiny::DiffusionMap( data = t( as.matrix( dge@data ) ) )
    dpt_out = destiny::DPT( dm )
    dge = add_pseudotime_to_seurat( dge, pt_obj = dpt_out, pt_method = "dpt" )
    saveRDS( dpt_out, file.path( results_path, "dpt_out.data" ) )
  } else {
    mobj = call_monocle_on_seurat( dge, results_path, monocle_params = mp, earliest_day = earliest_day, ... )
    dge = add_pseudotime_to_seurat( dge, pt_obj = mobj )
    saveRDS( mobj, file.path( results_path, "mobj_TECs.data" ) )
  }
  saveRDS( dge,  file.path( results_path, "dge_TECs.data" ) )
  
  # # Plot default genes
  bv = paste0("branch_viz_", 1:2)
  misc_summary_info ( dge, results_path )
  # save_feature_plots( dge, results_path )
  if( method!="PCA" ){
    misc_summary_info ( dge, results_path, axes = bv, axes_description = "monocle_branch_viz" )
    save_feature_plots( dge, results_path, axes = bv, axes_description = "monocle_branch_viz" )
  }
  save_feature_plots( dge, results_path, do_time_series = T  )
  
  return( dge )
}


#' Take a principal component (first by default) and return a list of results gained by interpreting that PC as a pseudotime axis.
#'
#' @param dge: Seurat object to be used. Should have variable genes and PCA fields already filled in. 
#' Jackstraw and t-SNE are optional but preferred.
#' @param orient_var: pseudotime is flipped to correlate positively with this. Anything available 
#' from `Seurat::FetchData(dge, orient_var)` is fair game... as long as it's numeric.
#' @param pc.use: which principal component to take
#' 
#' @value List with named elements:
#' `$dge`: Seurat object with a `pseudotime` metadatum filled in
#' `$gene_corrs`: dataframe with genes ordered by correlation with pseudotime.
#'    Columns are `gene`, `corr`, and (if available from Jackstraw) `p_value`.
pc_as_pt = function( dge, pc.use = 1, orient_var = "eday" ){
  
  # # Construct pseudotime
  pt = Seurat::FetchData(dge, paste0("PC", pc.use)) %>% vectorize_preserving_rownames
  
  # # Get correlations with genes and sort through them
  dge = ProjectPCA( dge, do.print = F )
  gene_corrs = dge@pca.x.full[, "PC1", drop = F] %>% vectorize_preserving_rownames %>% sort
  gene_corrs = data.frame( gene = names(gene_corrs), 
                           corr = gene_corrs ) 
  if( prod( dim( dge@jackStraw.empP.full ) ) > 0 ){
    # Need to index by gene because of earlier sorting
    gene_corrs$p_value = dge@jackStraw.empP.full[gene_corrs$gene, "PC1"]
  } 
  rownames(gene_corrs) = gene_corrs$gene
  
  # # Flip everything if necessary
  wrong_way = cor( pt, Seurat::FetchData( dge, orient_var )[[1]] ) < 0
  if( wrong_way ){ orient = function(x) -x } else { orient = function(x) x }
  pt                  %<>% orient
  dge@pca.rot   [, 1] %<>% orient
  dge@pca.x     [, 1] %<>% orient
  dge@pca.x.full[, 1] %<>% orient
  gene_corrs$corr %<>% orient
  
  # # Add pt as metadata and return
  dge = Seurat::AddMetaData( dge, pt, "pseudotime" )
  return( list( dge = dge, gene_corrs = gene_corrs ) )
}

#' Characterize genes by behavior over pseudotime, returning cluster assignments and effect sizes. 
#' 
#' @param dge should be a seurat object with a field "pseudotime". 
#' The field `dge@data` is accessed for expression levels -- for Eric's objects, the units will be log2(1+CP10K).
#' @param results_path is a character vector showing where to dump the output.
#' @param num_periods_initial_screen Cells are partitioned into this many pseudotime periods (equal number of cells in each).
#' They get averaged and each gene's "effect size" is the largest minus the smallest average.
#' @param prop_genes_keep Genes are ranked by effect size and the top prop_genes_keep*100% are sent into smoothers and kmeans.
#' @param abcissae_kmeans Gene expression is fed into k-means as a series of predictions at successive time points.
#' The arguments says how many time points to predict and feed in (if length one) or what time points (if longer).
#' @param num_clusters Genes are partitioned into this many modules. If NULL (default) the value is selected via the gap statistics and their SEs using the method in the original gap statistic paper.
#' 
#'       Tibshirani, R., Walther, G. and Hastie, T. (2001). Estimating the number 
#'       of data clusters via the Gap statistic. Journal of the Royal Statistical Society B, 63, 411423.
#'                                 
#' There's one adjustment: this function will never use just one cluster. It will issue a warning and use 2.
#'
#' @value A list with elements:
#'
#' - dge: the Seurat object
#' - gene_stats: genes with effect sizes and cluster labels.
#' - smoothers: fitted regression models, one for each gene.
#' - cluster_mod: output from stats::kmeans 
#' - gap_stats: output from cluster::clusGap
#'
#' @details This function helps explore gene dynamics over pseudotime. It goes through three main steps:
#'
#' - find genes that respond strongly to pseudotime.
#' - smooth those genes' expression to form an overall pseudotime trend.
#' - cluster genes based on smoothed expression patterns that have been shifted/scaled to the unit interval.
#'
smooth_and_cluster_genes = function( dge, results_path, 
                                     num_periods_initial_screen = 20, 
                                     prop_genes_keep = 0.1,
                                     abcissae_kmeans = 20,
                                     num_clusters = NULL ){
  
  # # Get data and average within epochs
  dir.create.nice( results_path )
  cat("Screening out bottom", 100*(1-prop_genes_keep), 
      "%, ranked by max minus min of average expression within", num_periods_initial_screen, "epochs ...\n")
  cell_data = Seurat::FetchData( dge , "pseudotime" )
  nbreaks = num_periods_initial_screen+1
  breaks = quantile(cell_data$pseudotime, probs = (1:nbreaks - 0.5)/nbreaks)
  cell_data$period = cut( x = cell_data$pseudotime, 
                          breaks = breaks, 
                          labels = as.character( 1:num_periods_initial_screen ),
                          include.lowest = T, ordered_result = T)
  dge = Seurat::AddMetaData( dge, cell_data[, "period"] %>% matrixify_preserving_rownames, "period" )
  genes_by_period = t( aggregate.nice( t(dge@data), by = cell_data$period, FUN = mean ) )
  nonzero = apply( genes_by_period, 1, function(x) any(x>0) )
  genes_by_period = genes_by_period[nonzero, ]
  min2max = function(x) (max(x) - min(x))
  effect_size = apply( genes_by_period, 1, min2max )
  cutoff = quantile( effect_size, 1-prop_genes_keep )
  genes_included = names(which(effect_size > cutoff )) 
  {
    pdf( file.path( results_path, "effect_sizes.pdf" ) )
    hist( effect_size, main = paste0("Max minus min average over ", num_periods_initial_screen, " periods"), breaks = 50 )
    abline( v = cutoff )
    dev.off()
  }
  
  cat("Smoothing gene expression over pseudotime...\n")
  pt_data = FetchData( dge, c( "pseudotime", genes_included ) )
  s = mgcv:::s
  fast_smooth = function( gene ) { 
    gene = pt_data[[gene]] 
    pseudotime = pt_data[["pseudotime"]]
    mgcv::gam( gene ~ s( pseudotime ), family = mgcv::nb() )
  }
  smoothers = lapply( genes_included, fast_smooth )
  names( smoothers ) = genes_included
  
  # # Generate kmeans features
  if( length( abcissae_kmeans ) == 0 ) { abcissae_kmeans = 20 }
  if( length( abcissae_kmeans ) == 1 ) {
    abcissae_kmeans = quantile(cell_data$pseudotime, (1:abcissae_kmeans - 0.5)/abcissae_kmeans )
  }
  kmeans_features = sapply( smoothers, predict, type = "response", 
                            newdata = data.frame( pseudotime = abcissae_kmeans ) )
  kmeans_features = t(kmeans_features)
  #rescale each gene to the unit interval
  kmeans_features = apply( kmeans_features, MARGIN = 1, FUN = function( x ) (x - min(x)) ) %>% t
  kmeans_features = apply( kmeans_features, MARGIN = 1, FUN = div_by_max ) %>% t
  atae(ncol(kmeans_features), length(abcissae_kmeans))
  atae(nrow(kmeans_features), length(genes_included))
  rownames(kmeans_features) = genes_included
  
  cat("  Calculating gap statistics... \n")
  gap_stats = cluster::clusGap( x = kmeans_features, FUNcluster = kmeans, K.max = 25, spaceH0 = "scaledPCA", 
                                iter.max = length(abcissae_kmeans) * 2 )
  {
    pdf( file.path( results_path, "gap_stats.pdf" ) )
    plot( gap_stats )
    dev.off()
  }
  if( is.null( num_clusters ) ) {
    num_clusters = cluster::maxSE(gap_stats$Tab[, 3], gap_stats$Tab[, 4], "Tibs2001SEmax") 
    if( num_clusters == 1 ){
      warning("Based on gap statistics, Tibshirani et alii suggest one cluster, but that's boring so I'll use two.")
      num_clusters = 2
    }
  }
  cat("  Continuing with ", num_clusters,  " clusters.\n")
  cluster_mod = kmeans( kmeans_features, centers = num_clusters, iter.max = 500 )
  
  # Reorder clusters by hierarchical clustering 
  peak_position = apply( cluster_mod$centers, 1, function( x ) mean( which( x > 0.75 ) ) )
  my_rf = function(hc) as.hclust(stats::reorder(as.dendrogram(hc), order(peak_position)))
  converter = dendrogram_merge_points( X = cluster_mod$centers, num_desired = 3,
                                       REORDER_FUN = my_rf,
                                       PLOT_FUN = function(...){ }, # skip plotting
                                       results_path = results_path, return_hc = F )
  hc        = dendrogram_merge_points( X = cluster_mod$centers, num_desired = 3, 
                                       REORDER_FUN = my_rf,
                                       PLOT_FUN = function(...){ }, # skip plotting
                                       results_path = results_path, return_hc = T )
  preferred_ordering = hc$order
  # image(cluster_mod$centers[preferred_ordering, ]) # Sneak peek for development
  
  old_given_new = preferred_ordering
  new_given_old = function( k ){ which(k==old_given_new)}
  cluster_mod$cluster  = sapply( cluster_mod$cluster, new_given_old )
  cluster_mod$centers  = cluster_mod$centers [old_given_new, ]
  cluster_mod$withinss = cluster_mod$withinss[old_given_new]
  cluster_mod$size     = cluster_mod$size    [old_given_new]
  rownames(cluster_mod$centers) = NULL
  
  # # Assemble data for export
  cat("Exporting cluster assignments and effect sizes for genes under study... \n")
  to_return = data.frame( gene = genes_included,
                          max_log2_fc = effect_size[genes_included], 
                          cluster = cluster_mod$cluster[genes_included] )
  to_return = to_return[order( to_return$cluster, -to_return$max_log2_fc ), ]
  write.table( to_return, file.path(results_path, "gene_pt_dependence_stats.txt"), 
               sep = "\t", quote = F, row.names = F, col.names = T)
  
  cat("Done.\n")
  return( list( dge = dge,
                # smoothing/filtering
                gene_stats = to_return,
                smoothers = smoothers, 
                # clustering
                abcissae_kmeans = abcissae_kmeans,
                kmeans_features = kmeans_features,
                cluster_mod = cluster_mod,
                gap_stats = gap_stats, 
                # dendrogram of cluster means
                converter = converter, 
                hc = hc ) )
}

#' Draw overlaid line plots of gene clusters from output of `smooth_and_cluster_genes`.
#'
#' @param facet_ncol For resulting facet plots, number of columns. 
facet_plot_gene_clusters = function( dge, results_path, 
                                     cluster_mod, 
                                     kmeans_features,
                                     abcissae_kmeans, 
                                     facet_ncol = NULL ){
  cluster_mod$cluster = factor( cluster_mod$cluster )
  num_clusters = length(levels(cluster_mod$cluster))
  centers = aggregate.nice( kmeans_features, by = cluster_mod$cluster, FUN = mean )
  data_wide = data.frame( rbind( centers, kmeans_features ) )
  data_wide$is_center = F; data_wide$is_center[1:num_clusters] = T
  data_wide$cluster = c( levels(cluster_mod$cluster), as.character( cluster_mod$cluster ) )
  data_wide$gene = rownames( data_wide )
  
  data_long = melt( data_wide, id.vars = c( "is_center", "cluster", "gene" ))
  data_long$unit_scaled_expression = data_long$value
  data_long$pseudotime = abcissae_kmeans[ data_long$variable ]
  data_long$cluster %<>% factor(levels = rev(sort(unique(data_long$cluster))), ordered = T)
  p_faceted_clusters = ggplot() + ggtitle( "Major gene clusters" ) +
    geom_line( data = subset( data_long, !is_center), alpha = 0.6,
               aes( x = pseudotime, y = unit_scaled_expression, group = gene, colour = cluster ) ) +
    geom_line( data = subset( data_long, is_center), alpha = 1, colour = "black",
               aes( x = pseudotime, y = unit_scaled_expression, group = gene  ) ) +
    facet_wrap( ~cluster, ncol = facet_ncol ) + 
    theme(strip.background = element_blank(), legend.position = "none") +
    ylab("Unit-scaled expression") + 
    scale_colour_grey()

  ggsave( filename = file.path(results_path, "faceted_gene_clusters.pdf"), 
          p_faceted_clusters, 
          width = 5, height = 6 )
  return( p_faceted_clusters )
}

#' Draw heatmaps of gene clusters from output of `smooth_and_cluster_genes`.
#'
#' @param dge Seurat object with raw data and pseudotime metadata used to train smoothers. 
#' If metadata `simple_branch` is present, function is hardwired to look for "mTEC", "cTEC", "branchpoint", and "progenitor"
#' and make a heatmap similar to figure 2 in http://dx.doi.org/10.1101/122531. 
#' @param results_path Where to save plots and files.
#' @param cluster_mod K-means output with cluster labels for the genes in `smoothers` and also with cluster centers.
#' @param smoothers List of regression models for the genes in `cluster_mod` and `gene_stats`.
#' @param gap_size White bars separating clusters are formed by adding fake genes. gap_size is how many fake genes per bar.
#' @param genes_use 
#' @param genes_to_label
heatmap_gene_clusters = function( dge, results_path, 
                                  cluster_mod, 
                                  smoothers,
                                  gap_size = NULL,
                                  genes_use = NULL,
                                  genes_to_label = NULL ){  
  cell_data = dge %>% FetchData("pseudotime")
  num_clusters = length( unique( cluster_mod$cluster ) )
  atae( names( smoothers ), names(cluster_mod$cluster) )
  
  if( is.null( genes_use ) ){
    genes_use = names(cluster_mod$cluster)
  } else {
    cluster_mod$cluster = cluster_mod$cluster[genes_use]
    smoothers = smoothers[genes_use]
  }
  
  # # set up wide-format data
  if( "simple_branch" %in% AvailableData( dge ) ){
    abcissae_heatmap = FetchData(dge, "pseudotime")[[1]] 
    #avoids exact duplicates; need to use these as dimnames
    abcissae_heatmap = abcissae_heatmap + rnorm( n = length(abcissae_heatmap), mean = 0, sd = 1e-8 )
    abcissae_heatmap %<>% sort
  } else {
    abcissae_heatmap = seq( min(cell_data$pseudotime), max(cell_data$pseudotime), length.out = 100 )
  }
  data_wide_heat = sapply( smoothers, predict, type = "response", 
                           newdata = data.frame( pseudotime = abcissae_heatmap ) ) 
  rownames(data_wide_heat) = abcissae_heatmap
  peak_expression = apply( data_wide_heat, 2, which.max ); names(peak_expression) = names(smoothers)
  data_wide_heat = apply(data_wide_heat, 2, standardize) %>% t %>% as.data.frame
  data_wide_heat$gene = names(smoothers)
  data_wide_heat$cluster = cluster_mod$cluster

  # # Add bars between clusters by adding dummy genes ranked last in each cluster
  if( is.null(gap_size) ){
    gap_size = ifelse( length(genes_use) < 50, 0, ceiling( length(smoothers) / 100 ) )
  }
  scaffold = rep( 1:nrow(cluster_mod$centers), each = gap_size )
  if( gap_size > 0 ){
    dummy_genes = matrix( NA, 
                          ncol = length( abcissae_heatmap ), 
                          nrow = length( scaffold ) ) %>% as.data.frame
    dummy_genes$gene = paste0("DUMMY_GENE_", scaffold ) %>% make.unique
    dummy_genes$cluster =                    scaffold
    colnames(dummy_genes) = colnames(data_wide_heat)
    data_wide_heat = rbind( data_wide_heat, dummy_genes )
    peak_expression = c( peak_expression, rep( Inf, length(scaffold) ) ) #used for ranking
  }
  
  # # Add vertical bar for branching heatmap
  if( "simple_branch" %in% AvailableData( dge ) ){
    pt_by_branch = aggregate.nice( x =  FetchData(dge, "pseudotime"), 
                                   by = FetchData(dge, "simple_branch"), 
                                   FUN = mean )
    atae( pt_by_branch["progenitor", ], 0, tol = 1e-8 )
    atae( pt_by_branch["branchpoint", ], 0, tol = 1e-8 )
    atat( pt_by_branch["mTEC", ] < 0 )
    atat( pt_by_branch["cTEC", ] > 0 )
    branch_counts = FetchData(dge, "simple_branch") %>% table
    boundary = branch_counts["mTEC"] + branch_counts["progenitor"] / 2 + branch_counts["branchpoint"] / 2
    dummy_cells = matrix(NA, nrow = nrow( data_wide_heat ), ncol = ncol( data_wide_heat )/40 )
    middle_pt = abcissae_heatmap[boundary + 0:1 ]
    colnames(dummy_cells) = seq( max(middle_pt) + 1e-8, 
                                 min(middle_pt) - 1e-8, length.out = ncol( dummy_cells ) )
    is_left = 1:ncol(data_wide_heat) < boundary
    data_wide_heat = cbind( data_wide_heat[,  is_left],
                            dummy_cells,
                            data_wide_heat[, !is_left] )
  }
  
  # # Melt data and order rows
  data_wide_heat$gene = factor( data_wide_heat$gene, 
                                levels = data_wide_heat$gene[order(data_wide_heat$cluster, -peak_expression)] , 
                                ordered = T)
  data_wide_heat = data_wide_heat[order(data_wide_heat$gene), ]
  data_long_heat = melt(data_wide_heat, id.vars = c("gene", "cluster"))
  data_long_heat %<>% rename( c("variable" = "pseudotime", "value" = "rel_log_expr") )

  # # Smoothed heatmap
  p_heat_clusters = ggplot(data = data_long_heat ) + 
    ggtitle( "Genes clustered by temporal expression pattern" ) +
    geom_raster( aes( x = pseudotime, y = gene, fill = rel_log_expr ), 
                 interpolate = T ) +
    scale_fill_gradientn( colors = blue_gray_red, na.value="white" ) + 
    theme(axis.line = element_blank(), axis.ticks = element_blank(), axis.text.x=element_blank()) 
  
  # # Label selected genes
  rest = setdiff( data_wide_heat$gene, genes_to_label )
  ylabels = c( genes_to_label, rep("", length(rest))  )
  names(ylabels) = c( genes_to_label, rest )
  p_heat_clusters = p_heat_clusters + 
    scale_y_discrete( labels = ylabels ) + labs( y = "Genes" ) 

  # # X axis is either by cells (branching) or by pseudotime (not branching).
  if( "simple_branch" %in% AvailableData( dge ) ){
    p_heat_clusters = p_heat_clusters + xlab("Cells")  
  } else {
    p_heat_clusters = p_heat_clusters + xlab("Pseudotime")
  }
  
  # # Add cluster colorbar
  boundary_genes = which(1==diff(sort(data_wide_heat$cluster)))
  boundary_genes = c(0, boundary_genes, length( data_wide_heat$gene ) ) 
  right_edge = max(as.numeric(data_long_heat$pseudotime))
  left_edge  = min(as.numeric(data_long_heat$pseudotime))
  for( i in 1:num_clusters ){
    p_heat_clusters = p_heat_clusters +
      annotate( "rect", 
                xmin = left_edge - 4 * (right_edge - left_edge) / 100,
                xmax = left_edge -     (right_edge - left_edge) / 100,
                ymin = boundary_genes[i  ] + gap_size,
                ymax = boundary_genes[i+1],
                fill = scales::hue_pal()(num_clusters)[i]) + 
      annotate( "text", 
                x = left_edge - 8 * (right_edge - left_edge) / 100, 
                y = (boundary_genes[i  ] + gap_size + boundary_genes[i+1]) / 2, 
                label = i) 
    
  }
  
  # # Add cell types
  branch_colors = c(scales::hue_pal()(2), "purple", "gray" )
  names(branch_colors) = c("mTEC", "cTEC", "progenitor", "branchpoint" )
  if( "simple_branch" %in% AvailableData( dge ) ){
    X = FetchData(dge, c("simple_branch", "pseudotime")) 
    X = X[order(X$pseudotime), ]
    X = X[["simple_branch"]] %>% as.character %>% rle #run-length encoding keeps from overwhelming ggplot
    X$positions = c(0, cumsum(X$lengths) )
    X$positions %<>% div_by_max
    X$positions = X$positions*right_edge
    for( i in 1:length(X$values) ){
      cell_label_data = 
        data.frame( 
          xmin = X$positions[i], 
          xmax = X$positions[i+1], 
          ymin = 2*ceiling( length(smoothers) / 100 ),
          ymax = 0 )
      p_heat_clusters = p_heat_clusters + 
        geom_rect( data = cell_label_data,
                   aes( xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax ), 
                   fill = branch_colors[X$values[i]] ) 
      
      dummy_cells_pseudotime = as.numeric(colnames(dummy_cells)) * right_edge / max(X$positions)
      p_heat_clusters = p_heat_clusters + 
        annotate( geom = "rect", fill = "white", 
                  xmin = min( dummy_cells_pseudotime ), 
                  xmax = max( dummy_cells_pseudotime ), 
                  ymin = 2*ceiling( length(smoothers) / 100 ), 
                  ymax = 0 ) 
    }                
  }

  ggsave( filename = file.path(results_path, "heatmapped_gene_clusters.png"), 
          p_heat_clusters, width = 6, height = 7 )
  ggsave( filename = file.path(results_path, "heatmapped_gene_clusters.pdf"), 
          p_heat_clusters, width = 6, height = 7 )
  return( p_heat_clusters )
}

#' Plot a dendrogram, labeling merges given by `converter` with colored rectangles. 
#'
#' @details If converter is c("a", "a", "b", "c"), you get edges labeled as a 
#' four-colour muted rainbow next to a grayscale with light, light, medium, and dark.
#' This is to convey that the first two tips have been somehow grouped.
 plot_dendro_with_rect = function( hc, converter, main = "Dendrogram" ){
    p = ggdendro::ggdendrogram(hc)
    p = p + annotate( geom = "tile", x = 1:length(hc$order), 
                      y = -2, 
                      fill = scales::hue_pal()(length(hc$order)) ) 
    p = p + annotate( geom = "tile", x = 1:length(hc$order), 
                      y = -1, 
                      fill = scales::grey_pal()(3)[factor(converter[hc$order])] ) 
    p = p + ggtitle(main)
    print(p)
    return(p)
  }
#' Plot a dendrogram and also cut it to merge input into `num_desired` groups.
#'
dendrogram_merge_points = function( X, num_desired, results_path, 
                                    FUN = function(x) hclust(dist(x), method = "ward.D2"), 
                                    REORDER_FUN = function(hc) as.hclust(stats::reorder(as.dendrogram(hc), 1:nrow(X))), 
                                    PLOT_FUN = plot_dendro_with_rect,
                                    CUT_FUN = stats::cutree,
                                    main = "Dendrogram",
                                    return_hc = F,
                                    ... ){
  hc = FUN( X, ... )
  hc = REORDER_FUN( hc )
  converter = CUT_FUN(hc, num_desired)
  converter = setNames( letters[converter], names(converter) )
  atae(as.character(hc$labels), names(converter))
  hc$labels = paste0( hc$labels, " (", converter[hc$labels], ")" ) 
  {
    pdf( file.path( results_path, paste0( main, ".pdf" ) ) )
    PLOT_FUN( hc, converter, main = main )
    dev.off()
  }
  PLOT_FUN( hc, converter, main = main )
  
  if(return_hc){
    return( hc )
  }
  return( converter )
}

```

```{r}
#' Extract data from a Seurat object and run Monocle, returning a Monocle object.
#'
#'
call_monocle_on_seurat = function( dge, results_path, monocle_params, earliest_day = NULL ){
  
  attach( monocle_params )

  # # Make (m)onocle (obj)ect
  raw_dge = deseuratify_raw_data( dge )
  geneInfo = data.frame( dge@mean.var[rownames(raw_dge), ] )
  geneInfo$gene = rownames(geneInfo)
  pd = new("AnnotatedDataFrame", data = dge@data.info[colnames(raw_dge), ])
  fd = new("AnnotatedDataFrame", data = geneInfo)
  mobj = newCellDataSet( as.matrix( raw_dge ), 
                         phenoData = pd, 
                         featureData=fd,
                         lowerDetectionLimit=1,
                         expressionFamily=negbinomial.size( ) )

  # # Select variable genes or get from Seurat object
  mobj = detectGenes( mobj, min_expr = 0 )
  mobj = estimateSizeFactors( mobj )
  mobj = estimateDispersions( mobj )
  if(reset_var_genes){
    disp_table = dispersionTable( mobj )
    ordering_genes = subset(disp_table,
                            ( mean_expression >= log_scale_expr_thresh ) & 
                              ( dispersion_empirical >= excess_disp * dispersion_fit ) )[["gene_id"]]
    cc_genes_go = read.table(file.path(PATH_TO_TABLES, "ms_cellcycleGO0007049.txt"),   sep="\t")
    cc_genes_macosko = get_macosko_cc_genes()
    cc_genes_all = Reduce(union, cc_genes_macosko) %>% union(., cc_genes_go)
    ordering_genes %<>% as.character %>% setdiff( cc_genes_all )
    expressed_genes = row.names(subset(fData(mobj), num_cells_expressed >= 10))
    ordering_genes = intersect(expressed_genes,ordering_genes)
    mobj = setOrderingFilter(mobj,ordering_genes)
  } else {
    mobj = setOrderingFilter(mobj, dge@var.genes )
  }

  # # Save variable genes and parameters
  vgsrp = file.path( results_path, "var_gene_select" )
  dir.create.nice( vgsrp )
  gd = mobj@featureData@data
  var_genes = gd$gene[gd$use_for_ordering]
  cell_markers = get_rene_markers()
  variable_cell_markers = intersect( Capitalize(cell_markers$marker), Capitalize(as.character(var_genes)) )
  variable_cell_markers = c(paste0(length(variable_cell_markers), "total"), variable_cell_markers)
  text2file( file.path( vgsrp, "markers_among_variable_genes_monocle.txt" ), variable_cell_markers )
  totalstring = paste(length(as.character(var_genes)), "total")
  var_genes = c(totalstring, as.character(var_genes))
  text2file(file.path(vgsrp, "variable_genes_monocle.txt"), var_genes)
  vsp        = c( excess_disp,   log_scale_expr_thresh)
  names(vsp) = c("excess_disp", "log_scale_expr_thresh")
  text2file(file.path(vgsrp, "var_gene_selection_params_monocle.txt"), collapse_by_name(vsp))
  
  
  # # Do Monocle dimension reduction, correcting for cell cycle but not batch effects 
  # # (For us, batch effects are often completely nested inside of embryonic day, 
  # # so we can't adjust for them without removing the temporal signal.)
  form_str = paste0( "~", paste0( CC_PHASES, collapse = " + " ) )
  mobj = reduceDimension(mobj,residualModelFormulaStr=form_str,pseudo_expr=1, reduction_method = reduction_method)
  mobj = orderCells(mobj, num_paths = num_mature_types)
  
  if ( is.null( earliest_day ) ) {
    earliest_day = phenoData(mobj)[["eday"]] %>% as.character %>% as.numeric %>% min
  } 

  # # This code roots the lineage tree using a horrible horrible hack!  
  # # Monocle crashes when you give orderCells a root_state that is an internal node of the ddrtree graph.
  # # I can't tell which f***ing nodes are internal. So I pick the one I want, then rank the states by
  # # their proximity to it in one of the 2d embeddings. I try rooting the tree at the best
  # # ones first, stopping when something works.
  pct_eq_10_5 = function(x) mean( x == earliest_day )
  eday  = phenoData(mobj)[["eday"]] %>% as.character %>% as.numeric
  state = phenoData(mobj)[["State"]] %>% as.character %>% as.numeric
  coords = t( mobj@reducedDimS )
  eday_by_state     = aggregate.nice( x = eday,   by = state, FUN = pct_eq_10_5 )
  centroid_by_state = aggregate.nice( x = coords, by = state, FUN = mean )
  root_state = which.max( eday_by_state ) 
  dist_sq_to_root = apply( centroid_by_state, 1, FUN = function(x) distance_sq(x, centroid_by_state[root_state,]) )
  names( dist_sq_to_root ) = rownames( centroid_by_state )
  dist_sq_to_root = sort( dist_sq_to_root, decreasing = F )
  for( i in seq_along( dist_sq_to_root ) ){
    candidate_state = names( dist_sq_to_root )[[i]]
    e = tryCatch( expr = { mobj = orderCells( mobj, root_state = candidate_state ); e = "success" },
                  error=function( err ){ "failure" } )
    if( e != "failure" ){
      break
    }
  }
  
  
  detach( monocle_params )

  return(mobj)  
}

# # Monocle often returns dozens of cell states.
# # This function simplifies them, compressing everything into the three branches
# # of a "Y" shape.
# # `mobj` is a CellDataSet object.
# # It uses a bunch of heuristics, because if I could do this properly, then I wouldn't need Monocle.
simplify_branches = function( mobj ){
  
  # # Fix the tips at the start of pseudotime, the farthest point from that, and the farthest point from 
  # # those two, where distance to the pair is the minimum over the individual distances.
  root_idx = which.min( mobj@phenoData@data[["Pseudotime"]] )
  root_embedding = mobj@reducedDimS[, root_idx]
  tip1_idx = which.max( apply( X = mobj@reducedDimS, MARGIN = 2, FUN = distance_sq, y = root_embedding ) )
  tip1_embedding = mobj@reducedDimS[, tip1_idx]
  min_dist_sq_other_tips = function(x) {
    d1 = distance_sq( x, tip1_embedding )
    d2 = distance_sq( x, root_embedding )
    return( min( d1, d2 ) )
  }
  tip2_idx = which.max( apply( X = mobj@reducedDimS, MARGIN = 2, FUN = min_dist_sq_other_tips ) )
  tip2_embedding = mobj@reducedDimS[, tip2_idx]
  tips = matrix(NA, ncol = 2, nrow = 3)
  tips[ 1, ] = root_embedding
  tips[ 2, ] = tip1_embedding
  tips[ 3, ] = tip2_embedding
  
  # # For fixed tips and a given center, return the total distance from each point
  # # to the nearest branch, where the branch is a ray terminating at the center and extending
  # # through the branch tip.
  # # Count only three cells per state, randomly selected.
  cells_counted = mobj@phenoData@data %>% rownames
  cells_counted = aggregate.nice( cells_counted, by = mobj@phenoData[["State"]], FUN = sample, size=3)
  cells_counted %<>% c
  get_branchmodel_objective = function( center, return_assignments = F ){
    get_distance_sq_to_branches = function( x ){
      return( c( apply( X = tips, MARGIN = 1, FUN = distance_sq_to_ray, tip2 = center, point = x) ) )
    }
    distances = apply(X = mobj@reducedDimS, MARGIN = 2, FUN = get_distance_sq_to_branches )
    assignments = apply(X = distances, MARGIN = 2, FUN = which.min)
    if( return_assignments ){
      return( assignments )
    }
    
    index_mat = cbind( assignments, 1:ncol(distances))
    index_mat = index_mat[ cells_counted, ]
    return( sum( sqrt( distances[index_mat] ) ) )
  }
  
  # # Place the branchpoint so as to minimize the measure of deviation defined above.
  # # Assign each cell to the nearest branch.
  all_cells = mobj@phenoData@data %>% rownames
  tip_names = all_cells[ c(root_idx, tip1_idx, tip2_idx) ]
  q1 = quantile( mobj@phenoData[["Pseudotime"]], 0.1 )
  q2 = quantile( mobj@phenoData[["Pseudotime"]], 0.9 )
  pt = mobj@phenoData[[ "Pseudotime"]]
  names(pt) = mobj@phenoData@data %>% rownames
  cells_in_middle = all_cells[ q1 < pt[all_cells] & pt[all_cells] < q2 ]
  candidates = sample( setdiff( cells_in_middle, tip_names ), size = 100, replace = F)
  sse_list = apply( X = mobj@reducedDimS[, candidates],
                    MARGIN = 2, 
                    FUN = get_branchmodel_objective )
  branch_point_barcode = names( which.min( sse_list ) )
  branch_point_embedding = mobj@reducedDimS[, branch_point_barcode]
  mobj@phenoData@data[["branch"]] = get_branchmodel_objective( branch_point_embedding, return_assignments = T )[ all_cells ]
  plot_df = data.frame( x = mobj@reducedDimS[1, ],
                        y = mobj@reducedDimS[2, ], 
                        State = as.factor( mobj@phenoData[["State"]] ), 
                        Branch = as.factor( mobj@phenoData[["branch"]] ) )
  
  # # Visual sanity check and return.
  p = ggplot( data = plot_df ) + 
    ggtitle("Simplified branch assignments") +
    geom_point( aes( x = x, y = y, colour = Branch ) ) + 
    geom_line( data = as.data.frame( rbind( branch_point_embedding, tips[1, ] ) ), aes( x = V1, y = V2 ) )  + 
    geom_line( data = as.data.frame( rbind( branch_point_embedding, tips[2, ] ) ), aes( x = V1, y = V2 ) )  + 
    geom_line( data = as.data.frame( rbind( branch_point_embedding, tips[3, ] ) ), aes( x = V1, y = V2 ) )  + 
    geom_point( data = plot_df[mobj@auxOrderingData$DDRTree$branch_points, ],  aes( x = x, y = y ), colour = "purple"  ) 
  print( p )
  return( mobj )
}

# # Calculates the distance between `point` and the closest vector of the form
# # `a( tip1 - tip2 ) + tip2` where a >= 0. 
# # Helper for `simplify_branches`.
# # All inputs should be numeric vectors of the same length.
distance_sq_to_ray = function( tip1, tip2, point ) {
  if( all( tip1 == point ) | all( tip2 == point ) ){ return(0) }
  point_centered = ( point - tip2 )
  tip1_centered  = ( tip1  - tip2 )
  point_centered_scaled  = point_centered / sqrt( distance_sq ( point_centered, 0 ) )
  tip1_centered_scaled   = tip1_centered  / sqrt( distance_sq ( tip1_centered,  0 ) )
  c_cos_theta = sum( tip1_centered_scaled * point_centered_scaled ) * sqrt( distance_sq ( point_centered, 0 ) )
  comp_parallel =  c_cos_theta * tip1_centered_scaled
  if( 0 < c_cos_theta ){
    return ( distance_sq( point_centered, comp_parallel ) )
  } else {
    endpoint_diffs = c( distance_sq ( point, tip2 ), 
                        distance_sq ( point, tip1 ) )
    return ( min ( endpoint_diffs ) )
  }
}

# # This function takes Seurat object and a finished object from a pseudotime analysis package
# # It transfers info from the latter to the former, guaranteeing that the Seurat object
# # will have complete metadata fields of the following names:
# # `pseudotime`, `branch`, `branch_viz_1`, `branch_viz_2`
add_pseudotime_to_seurat = function(dge, pt_obj, pt_method = "monocle" ){
  if( pt_method == "monocle" ){
    to_add = data.frame(
      branch_viz_1 = pt_obj@reducedDimS[1, dge@cell.names], 
      branch_viz_2 = pt_obj@reducedDimS[2, dge@cell.names],
      branch = as.character( pt_obj@phenoData[["State"]] ),
      pseudotime = pt_obj@phenoData[["Pseudotime"]]       ) 
    atae( dge@cell.names,  rownames( pt_obj@phenoData@data ) )
    rownames( to_add ) = dge@cell.names
  } else if (pt_method == "dpt") {
    to_add = data.frame(
      branch_viz_1 = pt_obj@dm@eigenvectors[, 1],
      branch_viz_2 = pt_obj@dm@eigenvectors[, 2],
      branch_viz_3 = pt_obj@dm@eigenvectors[, 3],
      branch = as.character( pt_obj@branch [, 1] ),
      pseudotime = pt_obj[, 1]        )
    to_add$pseudotime = to_add$pseudotime*sign(cor(to_add$pseudotime, FetchData(dge, "eday")))
    rownames( to_add ) = dge@cell.names
    warning("Removing root cell from DPT output due to weird gap from rest of dataset.")
    bad_idx = which( to_add$pseudotime == 0 )
    to_add = to_add[-bad_idx, ]
    dge = Seurat::SubsetData( dge, cells.use = rownames( to_add ) )
  } else {
    warning("Only transfer from monocle and dpt has been implemented. Returning seurat object untouched.")
  }
  dge = Seurat::AddMetaData(dge, to_add)

  return( dge )
}
```

#####Pseudotime helpers

```{r}

# # For bootstrap analysis where I call Monocle on subsets of the data, it's useful to 
# # align all of the results so that cells are in roughly the same positions across bootstrap samples. 
# # This program does that for any two matrices mat_ref and mat_samp.
# # Rows are cells and columns are dimensions of some embedding.
# # They can have different numbers of columns, if you really want, but if the reference has more columns 
# # than the sample you're aligning to it, the resulting matrix will be singular.
# # (If you're a linear algebra nerd, technically this could happen under other circumstances as well, but it's unlikely.)
# #
# # They should have the property `all( rownames( mat_samp ) %in% rownames( mat_ref ) )`, or
# # else they should have the same number of rows.
# #
# # The return value is a projection of each column of mat_ref onto the column space of the 
# # augmented matrix [mat_samp | 1]. In other terms, mat_samp is translated and linearly transformed to 
# # minimize the l2 distance to mat_ref.
# # This function always returns a numeric matrix with ncol = ncol(mat_ref) and nrow = nrow(mat_samp).
align_embedding_to_reference = function(mat_samp, mat_ref, do.plot = F){
  
  # # Handle vectors
  if( is.null( dim ( mat_samp ) ) ) { mat_samp = matrixify_preserving_rownames( mat_samp ) }
  if( is.null( dim ( mat_ref  ) ) ) { mat_ref  = matrixify_preserving_rownames( mat_ref  ) }
  
  # # Handle dataframes
  mat_samp = as.matrix( mat_samp )
  mat_ref  = as.matrix( mat_ref  )

  # # Check dimensions
  if( ncol( mat_samp ) > nrow( mat_samp ) ||
      ncol( mat_ref  ) > nrow( mat_ref  ) ){
    warning("Did you forget to transpose?")
  }
  
  # # Ensure rownames present and compatible between sample and reference
  if( is.null( rownames( mat_samp ) ) |  is.null( rownames( mat_ref ) ) ){
    if( nrow( mat_samp ) != nrow( mat_ref ) ){ 
      stop( "Either number of rows should be equal or rows should be named with `all( rownames( mat_samp ) %in% rownames( mat_ref ) )`." ) 
    } else {
      rownames( mat_samp ) = as.character( 1:nrow( mat_samp ) )
      rownames( mat_ref )  = as.character( 1:nrow( mat_samp ) )
    }
  }
  atat( !is.null( rownames( mat_samp ) ) )
  atat( !is.null( rownames( mat_ref  ) ) )
  atat( all( rownames( mat_samp ) %in% rownames( mat_ref ) ) )
  mat_ref = mat_ref[ rownames( mat_samp ) , , drop = F] # drop = F GODDAMMIT

  # # Do math
  mat_return = matrix( lm( mat_ref ~ mat_samp )$fitted.values, ncol = ncol( mat_ref ) )
  
  # # guarantee nice named-matrix output
  rownames( mat_return ) = rownames( mat_samp ) 
  atat( nrow( mat_return ) == nrow( mat_samp ) )
  atat( ncol( mat_return ) == ncol( mat_ref ) )
  atat( is.numeric( mat_return ) )
  
  # # For testing
  if(do.plot){
    mat_ref     = data.frame( mat_ref );    mat_ref$type    = "input_ref"
    mat_samp    = data.frame( mat_samp );   mat_samp$type   = "input_sample"
    mat_return  = data.frame( mat_return ); mat_return$type = "return_val"
    df_all = rbind( mat_ref, mat_samp, mat_return )
    p = ggplot(df_all) + ggtitle("return_val and input_ref should be aligned") +
      geom_point(aes(X1, X2, colour = type), alpha = 0.2)
    print(p)
  } 
  return( mat_return )
}

# # Brief test script
# # One dimensional test
# refscale = 1:5; names(refscale) = letters[1:5]
# atae( matrixify_preserving_rownames( refscale ), 
#       align_embedding_to_reference(mat_ref = refscale, mat_samp = -10*refscale + 4 ),
#       tolerance = 0.000001,
#       check.attributes = F)
# # # Two dimensional test
# noisy_l  = cbind(-99:100, c( rep(1, 100), 1:100 ) - 50 ) + matrix( 2*rnorm(400), ncol = 2 )
# rownames( noisy_l ) = make.unique( letters[(1:200 %% 25) + 1] )
# noisy_l_rot = noisy_l %*% matrix( c(1, 1, -1, 1), nrow = 2 ) + matrix( 2*rnorm(400), ncol = 2 )
# tmp = align_embedding_to_reference(mat_ref  = noisy_l,
#                                    mat_samp = noisy_l, 
#                                    do.plot = T)
# tmp = align_embedding_to_reference(mat_ref  = noisy_l,
#                                    mat_samp = noisy_l_rot, 
#                                    do.plot = T)



#' Using predefined genesets, form simple scores summarizing cTEC and mTEC transcriptomic signatures.
#' 
#' @param dge Seurat object
#' @value results are added via `AddMetaData` using names "cTEC_signature", "mTEC_signature".
#'
#' @details TEC = thymic epithelial cell; c = cortical; m = medullary
add_cTEC_mTEC_signatures = function( dge ){
  active_genes = get_cTEC_mTEC_genes()
  cTEC_genes = active_genes %>% subset(., cluster == "cTEC", select = "gene", drop = T) 
  mTEC_genes = active_genes %>% subset(., cluster == "mTEC", select = "gene", drop = T) 
  signatures = data.frame( cTEC_signature =  FetchData(dge, vars.all = cTEC_genes) %>% rowMeans, 
                           mTEC_signature =  FetchData(dge, vars.all = mTEC_genes) %>% rowMeans )
  dge = AddMetaData( dge, signatures )
  return( dge )
}


#' Remove intra-TEC doublets by finding cells with both a high cTEC signature and a high mTEC signature.
#'
#' @param dge Seurat object
#' @param results_path Plots are saved here.
#' @param mTEC_thresh, @param cTEC_thresh Cells are excluded if they exceed both thresholds.
#' If these are NULL, they are selected automatically. 
#' @value A list with names "dge", "mTEC_thresh", and "cTEC_thresh."
#' 
#'
#' @details For threshold selection, we isolate some nearly-pure cTECs by ranking with all cells on 
#' cTEC signatures and retaining the top pure_fraction_cTEC (default 20%).
#' Among those cells, we take the (1 - reject_rate) quantile (default 98th percentile), as the
#' rejection threshold. 
#' The same process is used for mTEC threshold selection, but pure_fraction_mTEC defaults to just 2%.
#' 
remove_TEC_doublets = function( dge, 
                                results_path,
                                mTEC_thresh = NULL, 
                                cTEC_thresh = NULL, 
                                pure_fraction_mTEC = 0.02, 
                                pure_fraction_cTEC = 0.2, 
                                reject_rate = 0.05 ) {
  dir.create.nice( results_path )
  dge %<>% add_cTEC_mTEC_signatures
  X = FetchData(dge, c("mTEC_signature", "cTEC_signature"))
  
  if( is.null( mTEC_thresh ) ){ 
    cTEC_purity_cutoff = quantile( X$cTEC_signature, p = (1 - pure_fraction_cTEC) )
    pure_cTECs = subset(X, cTEC_signature > cTEC_purity_cutoff )
    mTEC_thresh = quantile( pure_cTECs$mTEC_signature, (1-reject_rate) )
  }
  if( is.null( cTEC_thresh ) ){ 
    mTEC_purity_cutoff = quantile( X$mTEC_signature, p = (1 - pure_fraction_mTEC) )
    pure_mTECs = subset(X, mTEC_signature > mTEC_purity_cutoff )
    cTEC_thresh = quantile( pure_mTECs$cTEC_signature, (1-reject_rate) )
  }
  df_keep = subset( X, cTEC_signature < cTEC_thresh | mTEC_signature < mTEC_thresh )
  p = custom_feature_plot( dge, colour = NULL, 
                           axes = c( "mTEC_signature", "cTEC_signature" ) ) +
    ggtitle( paste0( "TEC doublet removal (", nrow( df_keep ), " of ", nrow( X ), " remain)" ) ) + 
    geom_hline( data = data.frame(cTEC_thresh), aes( yintercept = cTEC_thresh ), col = "red" ) +
    geom_vline( data = data.frame(mTEC_thresh), aes( xintercept = mTEC_thresh ), col = "red" ) 
  print( p )  
  ggsave( file.path( results_path, "tec_dub_cutoffs.pdf" ), plot = p, width = 7, height = 7 )
  cu = rownames( df_keep )
  dge = SubsetData( dge, cells.use = cu )
  return(list( dge = dge, 
               mTEC_thresh = mTEC_thresh, 
               cTEC_thresh = cTEC_thresh ) )
}
```

### Doublet detection and classification of new cells

```{r}
# # Given a named vector x with counts of various cell types, returns expected doublet quantities for each possible pairing.
# # Make sure you only feed this one replicate at a time! You can't get doublets across replicates.
# # Assumes a doublet rate of 5%. Your mileage (and flow rates) may vary.
# #
# # Output is a named numeric vector of expected cell counts. 
# # For names, every combination of names(x) should be present once in the output.
# # Order doesn't matter, so labels get alphabetized and concatenated with '_'.
# # Within-cluster doublets are included.
# # E.g. you get BLD_END and BLD_BLD but not END_BLD.
expected_doublet_counts = function( x, rate = 0.05 ){
  my_mat = matrix(x, nrow = length(x)) %*% matrix(x, ncol = length(x))
  my_mat = my_mat / sum( x )
  my_mat = my_mat*rate
  # Count BLD_TEC and TEC_BLD together
  for(i in 1:nrow(my_mat)){
    for(j in 1:ncol(my_mat)){
      if(i > j){ 
        my_mat[i, j] = 0
      }
      # adding this tiny bit is a dumb hack to prevent zeroes from disappearing during summary(Matrix(, sparse = T))
      if(i < j){ 
        my_mat[i, j] = 2*my_mat[i, j] + 0.000001 
      }
      if( i==j ){
        my_mat[i, j] = my_mat[i, j] + 0.000001 
      }

    }
  }
  colnames(my_mat) = names( x )
  rownames(my_mat) = names( x )
  dubs = summary( Matrix( my_mat, sparse = T ) ) 
  colnames(dubs) = c("cell_type_1", "cell_type_2", "n_dubs")
  dubs$cell_type_1 = names( x )[dubs$cell_type_1]
  dubs$cell_type_2 = names( x )[dubs$cell_type_2]
  dubs$n_dubs = round(dubs$n_dubs, 3)
  postprocess = function( s1_s2 ){ paste( sort( s1_s2), collapse = "_" ) }
  rownames( dubs ) =  apply( dubs[, 1:2], 1, postprocess )
  dubs = setNames( dubs$n_dubs, nm = rownames(dubs))
  return( dubs )
}

#' Classify cells from one Seurat object in terms of another Seurat object's identity field, with a "reject option" for unfamiliar cells. 
#' 
#' @param dge_train Cells to train classifier on. Seurat object.
#' @param dge_test Cells to be classified. Seurat object.
#' @param ident.use Identity variable to use for training labels.
#' @param vars.all List of raw genes/features to use. If possible, will be accessed 
#' through `FetchData`; in this case, should be numeric. For others, zeroes are filled in.
#' If NULL, uses variable genes from both `dge_train` and `dge_test`.
#' @param my_transform If `is.null(my_transform)` (default), then `my_transform` is the identity. 
#' if `my_transform` has the form "PCA_<integer>", then the `my_transform` is an unscaled <integer>-dimensional
#' PCA based on the training data. This option triggers special behavior for quantifying 
#' classifier badness, because NN will perform badly in a principal subspace.
#' If user-specified, `my_transform` should accept and return matrices where rows are cells.
#' @param badness Either "pc_dist" or "neighbor_dist" or `NULL`. If `NULL`, default depends on 
#' `my_transform`. You can't use "pc_dist" unless `my_transform` has the form "PCA_<integer>".
#' @param k Number of nearest neighbors to use. Default 25. 
#' @param reject_prop Expected rate of false rejections you're willing to tolerate
#' on held-out training instances. Default is 1/100. This is not honest if `my_transform`
#' is chosen using the training data, and it cannot account for batch effects.
#' @value Seurat object identical to `dge_test` but with new/modified fields for 
#' - `classifier_ident` (predicted class) 
#' - `classifier_badness` (lower means higher confidence)
#' - `classifier_probs_<each identity class from trainset>` (predicted class probabilities)
#' @details Using k-nearest neighbors, classify cells from `dge_test` in terms of 
#' the options in `unique(FetchData(dge_train, ident.use))`, plus a reject option.
#' Rejection happens when the badness (usually distance to the nearest neighbors)
#' falls above a threshold (see `reject_prop`). Badness gets adjusted by cluster,
#' because some clusters naturally are less concentrated on the principal subspace
#' or the coordinates of interest.
knn_classifier = function( dge_train, dge_test, ident.use = "ident", 
                           vars.all = NULL, my_transform = NULL, badness = NULL,
                           k = 25, reject_prop = 0.01 ){
  if( is.null( vars.all ) ){ 
    vars.all = union( dge_train@var.genes, dge_test@var.genes ) 
  }
  
  # # Retrieve data, padding test data with zeroes as needed
  coords_train_orig = FetchDataZeroPad( dge_train, vars.all )
  coords_test_orig  = FetchDataZeroPad( dge_test,  vars.all )
  
  # # set the transform (and badness) 
  if( is.null( my_transform ) ){ my_transform = function(x) x }
  if( is.character( my_transform ) ) { 
    pca_n = strsplit( my_transform, "_", fixed = T )[[1]] 
    atae( "PCA",                  pca_n[1] )
    atat( !is.na( as.numeric( pca_n[2] ) ) )
    cat("Training transform: unscaled PCA of dimension", pca_n[2], " ...\n")
    w = irlba::prcomp_irlba( x = coords_train_orig, n = as.numeric( pca_n[2] ), 
                             retx = F,
                             center = colMeans( coords_train_orig ), 
                             scale = F )
    my_transform = function(x){
      sweep( x, 
             STATS = w$center, 
             FUN = "-",
             MARGIN = 2 ) %*% w$rotation 
    }
    if( is.null( badness ) ){ badness = "pc_dist" }
  } else {
    if( is.null( badness ) ){ badness = "neighbor_dist" }
  }
  if ( badness == "pc_dist" ) {
    cat(" Using distance to principal subspace as badness. \n")
    get_badness = function( nn_dists, x ){ 
      z = sweep( x, 
                 STATS = w$center, 
                 FUN = "-",
                 MARGIN = 2 ) 
      zproj = z %*% w$rotation %*% t( w$rotation )
      square = function( x ) x^2 
      ( z-zproj ) %>% apply( 1, square ) %>% apply( 2, sum ) %>% sapply( sqrt )
    }
  } else {
    cat(" Using average distance to neighbors as badness. \n")
    get_badness = function( nn_dists, x ) { rowMeans( nn_dists )}
  }
  
  cat("Transforming data...\n")
  coords_train_trans = my_transform( as.matrix( coords_train_orig ) ) %>% as.data.frame
  coords_test_trans  = my_transform( as.matrix( coords_test_orig ) ) %>% as.data.frame
  
  # # Find nearest neighbors & classify
  cat("Finding nearest neighbors...\n")
  nn_out = FNN::get.knnx( data = coords_train_trans, 
                          query = coords_test_trans,
                          k=k, algorithm=c( "cover_tree" ) )
  train_labels = FetchData( dge_train, ident.use )[[1]]
  get_label = function(idx) train_labels[idx]
  empir_prob = function(x) factor( x, levels = unique( dge_train@ident ) ) %>% table %>% div_by_sum
  classifier_probs = apply( apply( nn_out$nn.index, 2, get_label ), 1, FUN = empir_prob ) %>% t %>% as.data.frame
  classifier_ident = apply( classifier_probs, 1, function(x) names( which.max( x ) ) )
    
  # # Get badness 
  cat("Calculating badness for each point...\n")
  nn_out_self = FNN::get.knn( data = coords_train_trans,
                              k=k, algorithm=c( "cover_tree" ) )
  classifier_prob_self = apply( apply( nn_out_self$nn.index, 2, get_label ), 
                                1, FUN = empir_prob ) %>% t %>% as.data.frame

  classifier_badness      = get_badness( nn_dists = nn_out$nn.dist,      x = as.matrix( coords_test_orig  ) )
  classifier_badness_self = get_badness( nn_dists = nn_out_self$nn.dist, x = as.matrix( coords_train_orig ) )
  
  # # Adjust badness via regression: some clusters are naturally less dense or farther from PC axes
  model_badness_by_cluster = lm( classifier_badness_self ~ . + 0, 
                                 data = cbind( classifier_badness_self, classifier_prob_self ) )  
  classifier_badness_self = classifier_badness_self - predict( object = model_badness_by_cluster ) 
  classifier_badness      = classifier_badness      - predict( object = model_badness_by_cluster, 
                                                               newdata = classifier_probs ) 
  classifier_badness      = classifier_badness      / sd(classifier_badness_self)
  classifier_badness_self = classifier_badness_self / sd(classifier_badness_self)
  
  # # Set threshold and label rejects
  cat("Labeling rejects with attempted controls on false rejection rate ... \n")
  threshold = quantile( classifier_badness_self, 1 - reject_prop )
  hist( classifier_badness_self,  breaks = 80, col = scales::alpha("blue", 0.5))
  hist( classifier_badness,       breaks = 80, col = scales::alpha("red", 0.5),
        add = T, 
        main = "Held-out set badness and threshold",
        xlab = "Average distance to neighbors (train = blue, test = red)" )
  abline( v = threshold )
  text( x = threshold*1.05, y = 100, labels = "Reject", srt = 90 )
  text( x = threshold*0.95, y = 100, labels = "Classify", srt = 90 )

  classifier_ident[classifier_badness >= threshold] = "reject"
  classifier_ident %>% table
  
  # # Save data and return
  to_add = cbind(          classifier_ident,   
                           classifier_badness,            
                           classifier_probs )
  colnames( to_add ) = c( "classifier_ident", 
                          "classifier_badness" , 
                          "classifier_probs_" %>% paste0( colnames( classifier_probs ) ) )
  rownames( to_add ) = rownames( coords_test_orig )
  dge_test %<>% AddMetaData( to_add )
  
  cat("Done.\n")
  return( dge_test )
}

# # Trains and saves a penalized logistic regression classifier.
# # Uses Seurat::FetchData(training_dge, vars.all = ident.use ) as class labels.
# # Results (`glmnet` object) and training data (Seurat object) get
# # saved into a subdirectory of `results_path`. 
train_save_classifier = function(training_dge, results_path, ident.use = "cell_type", genes.use, do.save = F ){
  
  # # Get labels
  training_labels = factor( vectorize_preserving_rownames ( Seurat::FetchData(training_dge, vars.all = ident.use ) ) )

  # # Get features
  genes.use = intersect( genes.use, rownames( training_dge@data ) )
  features_tf = t( training_dge@data[ genes.use, ] )

  # # Build classifier (penalized multinomial logistic regression)
  print("Training classifier...")
  # # alpha = 0 does L2 regression. alpha = 1 does LASSO. In between is elastic net.
  mlr_mod = glmnet::cv.glmnet(x = features_tf, y = training_labels, family = "multinomial", alpha = 0)
  print("... classifier trained.")
  
  # # Save and return
  if(do.save){
    dir.create.nice( file.path( results_path, "classifier" ) )
    print("Saving classifier...")
    saveRDS(mlr_mod,      file.path( results_path, "classifier", "glmnet_object.data" ) )
    saveRDS(training_dge, file.path( results_path, "classifier", "training_data.data" ) )
  }
  return( mlr_mod )
}

get_classifier_coefs = function( mlr_mod ){
  cell_types = names( coefficients( mlr_mod, newx = features, s = "lambda.min" ) )
  genes = rownames( coefficients( mlr_mod, newx = features, s = "lambda.min" )[[1]] )
  coeffs = data.frame( matrix( NA, nrow = length(genes), ncol = length( cell_types ) ) )
  colnames( coeffs ) = cell_types
  rownames( coeffs ) = make.unique( genes )
  for( cell_type in cell_types ){
    coeffs[[cell_type]] = as.vector( coefficients( mlr_mod, newx = features, s = "lambda.min" )[[cell_type]] )
  }
  return( coeffs )
}

#' Apply a machine learning classifier (from `train_save_classifier`) to new data
#' 
#' @param dge a Seurat object 
#' @param mlr_mod a glmnet multiclass logistic regression model.
#' You can feed the output of `train_save_classifier` into the `mlr_mod` argument.
classify_mlr = function( dge, mlr_mod ){
  genes_used = ( mlr_mod %>% coef %>% down_idx %>% attributes )$Dimnames %>% down_idx
  genes_used = setdiff( genes_used, "(Intercept)" )
  features = FetchDataZeroPad( dge, vars.all = genes_used ) %>% t
  predictions = predict( mlr_mod, newx = features, s = "lambda.min", type = "response" )[, , 1]
  return( predictions )
}

#' Visualize probabilistic classification results
#'
#' @param dge a Seurat object
#' @param results_path folder to place output in
#' @param mlr_mod a glmnet multiclass logistic regression model. Give this or `class_labels`, not both.
#' You can feed the output of `train_save_classifier` into the `mlr_mod` argument.
#' @param class_labels atomic character vector naming columns in the Seurat object that contain
#'  class probabilities. Give this or `mlr_mod`, not both. If names(class_labels) is filled in,
#'  then that's how the spokes get labeled.
#' @param fig_name filename for output.
#' @param facet_by Variable in Seurat object to facet resulting plots by; default is none
#' @param colour_by Variable in Seurat object to map color to; default is none
#' @param fig_name filename for output.
#' @param style If "points", plot each cell as a dot. 
#' If "density", then instead of plotting points, plot 2d density contours.
#' If "hexagons", do AWESOME HEXAGON BINNING YEAHHHHHHH HEXAGONS.
#' @param wheel_order Deprecated.
#' @param do.density Deprecated.
wheel_plot = function( dge, results_path, mlr_mod = NULL, class_labels = NULL, fig_name, 
                       colour_by = NULL, facet_by = NULL, style = "points",
                       wheel_order = NULL, do.density = NULL ){
  # # Handle stupid old input
  if(!is.null(wheel_order)){
    warning( "`wheel_order` arg is deprecated. Use `class_labels` instead." )
    if( is.null( class_labels ) ){ class_labels = wheel_order} 
  }
  
  if( !is.null(do.density) && do.density){ 
    warning("do.density is deprecated. Use ` style = \"density\" ` instead.")
    style = "density"
  }
  
  # # Handle regular, non-stupid input
  mlr_mod_given = !is.null( mlr_mod )
  labels_given = !is.null( class_labels )
  if( !mlr_mod_given & !labels_given ){
    stop("Please specify either `mlr_mod` or `class_labels`")
  } else if( !mlr_mod_given & labels_given ) {
    cat( "Fetching stored predictions from Seurat object.\n" )
    predictions = FetchData( dge, vars.all = class_labels ) %>% as.matrix
  } else if( mlr_mod_given & !labels_given ) {
    cat( "Making predictions from glmnet object.\n" )
    predictions = classify_mlr( dge, mlr_mod )
    class_labels = colnames( predictions )
  } else if( mlr_mod_given & labels_given ) {
    cat( "Making predictions from glmnet object.\n" )
    warning( "Since `mlr_mod` was given, using `class_labels` 
             only to order wheel spokes, not to fetch stored predictions." )
    predictions = classify_mlr( dge, mlr_mod )
    class_labels = colnames( predictions )
  }
  
  # # Make wheel
  lp1 = length( class_labels ) + 1 
  if( is.null( names( class_labels ) ) ){ names( class_labels ) = class_labels }
  unwrapped_circle = seq( 0, 2*pi, length.out = lp1 )
  wheel_spokes = data.frame( z = names( class_labels ), 
                             x = cos( unwrapped_circle[-lp1] ), 
                             y = sin( unwrapped_circle[-lp1] ) )
  # # Process data
  cat("Processing data.\n")
  cell_positions = predictions %*% as.matrix( wheel_spokes[, c("x", "y")] )
  cell_positions = as.data.frame( cell_positions );   colnames( cell_positions ) = c( "x", "y" )
  if( !is.null( colour_by ) ){
      cell_positions[[colour_by]] = Seurat::FetchData(dge, vars.all = colour_by)[ rownames( predictions ),  ] 
  }
  if( !is.null( facet_by ) ){
    cell_positions[[facet_by]] = Seurat::FetchData(dge, vars.all = facet_by )[ rownames( predictions ),  ] 
  }
  
  # # Avoid possible namespace conflict between class_labels in model parameters and class_labels in test data
  if( "class_labels" %in% c( facet_by, colour_by ) ){
    wheel_spokes$class_labels_in_model = wheel_spokes$class_labels
    wheel_spokes$class_labels = NULL
  }
  
  # # Add wheel & label spokes
  wheel_plot = ggplot()   + geom_path ( data = wheel_spokes,   mapping = aes(     x,     y ) )
  wheel_plot = wheel_plot + geom_text ( data = wheel_spokes,   mapping = aes( 1.2*x, 1.2*y, label = z ) )
  # # Add data & facet
  if( style == "density"){
    cat( "Estimating density.\n" )
    wheel_plot = wheel_plot + geom_density_2d( data = cell_positions, 
                                               mapping = aes_string( "x", "y", colour = colour_by ) )
  } else if( style == "hexagons"){
    if( !is.null( colour_by ) ) {
      warning( "Won't use color with style==\"hegaxons\" because fill is mapped to bin count. " )
    }
    wheel_plot = wheel_plot + geom_hex( data = cell_positions, mapping = aes_string( "x", "y" ) ) 
    wheel_plot = wheel_plot +  scale_fill_gradient(trans = "log")
  } else {
    wheel_plot = wheel_plot + geom_point( data = cell_positions, 
                                          mapping = aes_string( "x", "y", colour = colour_by ), alpha = 0.5 ) 
  }
  wheel_plot = wheel_plot + ggtitle( fig_name )
  if( !is.null( colour_by )  && is.numeric( cell_positions[[colour_by]] ) ){
    wheel_plot = wheel_plot + scale_color_gradientn( colours = blue_gray_red )
  }
  if ( !is.null( facet_by ) ){
      wheel_plot = wheel_plot + facet_wrap( as.formula( paste( "~", facet_by ) ) )  
  }
  
  # # Save & return
  cat( "Done. Saving and returning plot.\n" )
  ggsave( filename = file.path( results_path, paste0( fig_name, ".pdf" ) ),
          plot = wheel_plot,
          width = 12, height = 10)
  return( wheel_plot )
}
```

###Receptor-ligand screening

This function takes an array `is_expressed` with cell types in the colnames, genes in the rownames, and boolean values indicated whether than cell type expresses that gene. It saves results from cross-referencing the expression data with
a list of receptor-ligand pairs.

```{r}
screen_receptor_ligand = function( is_expressed, results_path ){
  
  # # Get receptor-ligand pairs; annotate with tissues expressed; save
  ramilowski = get_ramilowski()
  ramilowski$Ligand.ApprovedSymbol = NULL
  ramilowski$Receptor.ApprovedSymbol = NULL
  ramilowski = subset( ramilowski, ligand_mouse %in% rownames(is_expressed) )
  ramilowski = subset( ramilowski, receptor_mouse %in% rownames(is_expressed) )
  ramilowski$ligand_cell_types = 
    is_expressed[ramilowski$ligand_mouse, ] %>% 
    apply( 1, which ) %>% 
    sapply( names ) %>% 
    sapply( paste, collapse = "_")
  ramilowski$receptor_cell_types = 
    is_expressed[ramilowski$receptor_mouse, ] %>% 
    apply( 1, which ) %>% 
    sapply( names ) %>% 
    sapply( paste, collapse = "_")
  write.table( ramilowski, file =  file.path( results_path, "Receptor_ligand_all.txt" ), 
               sep = "\t", row.names = F, col.names = T, quote = F )

  absent = union( ramilowski$ligand_mouse, ramilowski$receptor_mouse ) %>% setdiff( rownames( is_expressed ) )
  if( length( absent ) > 0 ){
    zeropad = matrix(F, ncol = is_expressed, nrow = length( absent ), 
                     dimnames = list( gene = absent, 
                                      celltype = colnames(is_expressed)) )
    is_expressed %<>% rbind( zeropad )
  }
  
  # # Get lists of receptors and ligands for each tissue pairing
  num_unique_ligands = matrix( 0, nrow = 3, ncol = 3, 
                               dimnames = list( lig_expr_tissue = c("BLD", "MES", "TEC"),
                                                rec_expr_tissue = c("BLD", "MES", "TEC") ) )
  dir.create.nice( file.path( results_path, "ligand_lists" ) )
  dir.create.nice( file.path( results_path, "receptor_lists" ) )
  for( lig_tissue in rownames(num_unique_ligands)){
    for( rec_tissue in colnames(num_unique_ligands)){
      eligible_subset = subset( ramilowski, 
                                is_expressed[ligand_mouse,   lig_tissue] & 
                                  is_expressed[receptor_mouse, rec_tissue] )
      num_unique_ligands[lig_tissue, rec_tissue] = length( unique( eligible_subset$ligand_mouse ) )
      write.table( unique(eligible_subset$ligand_mouse  ), row.names = F, col.names = F, quote = F,
                   paste0( results_path, "/ligand_lists/"  , lig_tissue, "_to_", rec_tissue, ".txt") )
      write.table( unique(eligible_subset$receptor_mouse), row.names = F, col.names = F, quote = F,
                   paste0( results_path, "/receptor_lists/", lig_tissue, "_to_", rec_tissue, ".txt") )
      
    }  
  }
  
  # Background lists composed of everything that got successfully converted to mouse ortholog
  # For pathway analysis with a background list
  ramilowski_orig = read.table( file.path( PATH_TO_TABLES, "LigandReceptor_Ramilowski2015_mouse.txt" ), 
                                header = T, sep="\t", stringsAsFactors = F )
  write.table( ramilowski_orig$ligand_mouse   %>% unique, 
               paste0( results_path, "/ligand_lists/background.txt"),
               row.names = F, col.names = F, quote = F)
  write.table( ramilowski_orig$receptor_mouse %>% unique, 
               paste0( results_path, "/receptor_lists/background.txt"),
               row.names = F, col.names = F, quote = F)
  
  
  # # Save summary to file
  # # Sink helps get the full dimnames
  sink( file.path( results_path, "num_unique_ligands.txt" ) )
  {
    print( num_unique_ligands )
  }
  sink()
  return()
}
```

###Master functions

```{r}

# # Quickly explore many analysis options
explore_embeddings = function(dge, results_path, all_params, test_mode = F){
  atat(is.data.frame( all_params ) )
  required_params = c( "cc_method",
                       "num_pc", 
                       "clust_granularities_as_string",
                       "plot_all_var_genes" )
  atat( all( required_params %in% names( all_params ) ) )
  atat( any( c( "excess_var_cutoff","log_expr_cutoff", "prop_genes_to_select" ) %in% names( all_params ) ) )
  
  # # Record the things you're gonna try
  dir.create.nice( results_path )
  write.table( all_params, file = file.path(results_path, "params_to_try.txt"), 
               quote = F, sep = "\t", row.names = F)
  
  # # Try all the things
  prev_param_row = NULL
  for(i in rownames( all_params ) ){
    param_row = all_params[i, ]; names(param_row) = names(all_params)
    print("Trying these settings:")
    print(param_row)
    rp_mini = file.path(results_path, collapse_by_name( all_params[i,] ))
    dir.create.nice(rp_mini)

    # # remove cc variation
    if( "extra_regressout" %in% names( param_row ) ){
      extra_vars = trimws( strsplit( param_row[["extra_regressout"]], "," )[[1]] )
    } else {
      extra_vars = c()
    }
    if(is.null(prev_param_row) || param_row[["cc_method"]] != prev_param_row[["cc_method"]]){
      cc_scores_out = add_cc_score(dge, method = param_row[["cc_method"]])
      dge = Seurat::RegressOut(object = cc_scores_out$dge, 
                               latent.vars = c(cc_scores_out$cc_score_names, extra_vars))
    }
    # # select genes; do dim red; cluster cells
    dge = var_gene_select( dge, results_path = rp_mini, test_mode,
                           excess_var_cutoff   = param_row[["excess_var_cutoff"]],
                           log_expr_cutoff     = param_row[["log_expr_cutoff"]],
                           prop_genes_to_select = param_row[["prop_genes_to_select"]],
                           method = param_row[["var_gene_method"]])
    if( !is.null( param_row[[ "TF_only" ]] ) && param_row[[ "TF_only" ]] ){
      dge@var.genes %<>% intersect(get_mouse_tfs())
    }

    dge = Seurat::PCA(dge, pc.genes = dge@var.genes, do.print = F) 
    pc.use = 1:param_row[["num_pc"]]
    dge = Seurat::RunTSNE(dge, dims.use = pc.use, do.fast = T) 
    dge = cluster_wrapper(dge, results_path = rp_mini, test_mode = test_mode, 
                          method = param_row[["clust_method"]],
                          granularities_as_string = param_row[["clust_granularities_as_string"]],
                          pc.use = 1:param_row[["num_pc"]])
                             
    # # save plots and summaries 
    misc_summary_info( dge, results_path = rp_mini)
    saveRDS( dge, file.path( rp_mini, "dge.data") ) 
    
    top_genes_by_pc(   dge, results_path = rp_mini, test_mode)
    fm = param_row[["find_markers_thresh"]]
    if( !is.null( fm ) ){
      de_genes = find_de(dge, results_path = rp_mini, ident.use = "ident", thresh.use = fm )
      top_markers = get_top_de_genes(de_genes, results_path = rp_mini, 
                                     test_mode = test_mode, 
                                     top_n = 10, thresh_df = NULL)
      save_feature_plots(dge, results_path = rp_mini, gene_list = top_markers$gene, gene_list_name = "top_markers")
      # save_heatmap(dge, results_path = rp_mini, marker_info = de_genes,    main = "heatmap_all_de_genes")
      save_heatmap(dge, results_path = rp_mini, marker_info = top_markers, main = "heatmap_top_markers")
    }
    save_feature_plots(dge, results_path = rp_mini)
    pavg = param_row[["plot_all_var_genes"]]
    if( !is.null( pavg ) && pavg  ){
      save_feature_plots(dge, results_path = rp_mini, gene_list = dge@var.genes, gene_list_name = "var_genes")
    }
    prev_param_row = param_row
  }
  return(dge)
}

```

